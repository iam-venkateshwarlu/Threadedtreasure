
==> Audit <==
|---------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |   User    | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|-----------|---------|---------------------|---------------------|
| start   |                                | minikube | pradyuman | v1.36.0 | 16 Aug 25 20:24 IST | 16 Aug 25 20:25 IST |
| stop    |                                | minikube | pradyuman | v1.36.0 | 16 Aug 25 22:28 IST | 16 Aug 25 22:28 IST |
| start   |                                | minikube | pradyuman | v1.36.0 | 17 Aug 25 07:21 IST | 17 Aug 25 07:22 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 17 Aug 25 07:27 IST | 17 Aug 25 07:27 IST |
| stop    |                                | minikube | pradyuman | v1.36.0 | 17 Aug 25 13:34 IST | 17 Aug 25 13:35 IST |
| start   |                                | minikube | pradyuman | v1.36.0 | 18 Aug 25 07:44 IST |                     |
| start   |                                | minikube | pradyuman | v1.36.0 | 18 Aug 25 07:45 IST |                     |
| start   |                                | minikube | pradyuman | v1.36.0 | 18 Aug 25 07:46 IST | 18 Aug 25 07:46 IST |
| start   |                                | minikube | pradyuman | v1.36.0 | 18 Aug 25 17:44 IST |                     |
| start   |                                | minikube | pradyuman | v1.36.0 | 18 Aug 25 17:44 IST | 18 Aug 25 17:47 IST |
| start   |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 10:48 IST | 19 Aug 25 10:48 IST |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 10:54 IST | 19 Aug 25 10:56 IST |
|         | threaded-dev                   |          |           |         |                     |                     |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 10:56 IST |                     |
|         | threadedtreasures --url        |          |           |         |                     |                     |
| service | list                           | minikube | pradyuman | v1.36.0 | 19 Aug 25 10:57 IST | 19 Aug 25 10:57 IST |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 11:23 IST | 19 Aug 25 11:23 IST |
|         | threaded-dev --url             |          |           |         |                     |                     |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 11:25 IST | 19 Aug 25 11:27 IST |
|         | threaded-dev --url             |          |           |         |                     |                     |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 11:27 IST | 19 Aug 25 11:38 IST |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 11:53 IST | 19 Aug 25 12:05 IST |
|         | threaded-dev                   |          |           |         |                     |                     |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 12:03 IST | 19 Aug 25 12:03 IST |
|         | threaded-dev                   |          |           |         |                     |                     |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 12:04 IST |                     |
| ip      |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 12:06 IST | 19 Aug 25 12:06 IST |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 12:30 IST | 19 Aug 25 13:24 IST |
|         | threaded-dev                   |          |           |         |                     |                     |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:04 IST | 19 Aug 25 13:04 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:04 IST | 19 Aug 25 13:05 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:05 IST | 19 Aug 25 13:07 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:19 IST | 19 Aug 25 13:19 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:24 IST | 19 Aug 25 14:29 IST |
| service | threadedtreasures-service -n   | minikube | pradyuman | v1.36.0 | 19 Aug 25 13:24 IST | 19 Aug 25 13:29 IST |
|         | threaded-dev                   |          |           |         |                     |                     |
| tunnel  |                                | minikube | root      | v1.36.0 | 19 Aug 25 14:30 IST | 19 Aug 25 15:39 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:01 IST | 19 Aug 25 15:01 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:06 IST | 19 Aug 25 15:06 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:39 IST | 19 Aug 25 15:39 IST |
| stop    |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:53 IST | 19 Aug 25 15:54 IST |
| start   | --vm-driver=hyperkit           | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:54 IST |                     |
| delete  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:54 IST | 19 Aug 25 15:54 IST |
| start   |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:54 IST |                     |
| delete  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:55 IST | 19 Aug 25 15:55 IST |
| start   | --vm-driver=hyperkit           | minikube | pradyuman | v1.36.0 | 19 Aug 25 15:55 IST |                     |
| start   | --driver=qemu                  | minikube | pradyuman | v1.36.0 | 19 Aug 25 16:12 IST |                     |
| delete  | qemu                           | minikube | pradyuman | v1.36.0 | 19 Aug 25 16:17 IST |                     |
| delete  |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 16:17 IST | 19 Aug 25 16:17 IST |
| delete  | --all                          | minikube | pradyuman | v1.36.0 | 19 Aug 25 16:23 IST | 19 Aug 25 16:23 IST |
| start   | --driver=docker                | minikube | pradyuman | v1.36.0 | 19 Aug 25 16:24 IST | 19 Aug 25 16:24 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 17:28 IST | 19 Aug 25 17:28 IST |
| stop    |                                | minikube | pradyuman | v1.36.0 | 19 Aug 25 17:33 IST | 19 Aug 25 17:33 IST |
| start   | --driver=docker --cpus=4       | minikube | pradyuman | v1.36.0 | 22 Aug 25 12:09 IST |                     |
|         | --memory=81912                 |          |           |         |                     |                     |
| start   | --driver=docker --cpus=4       | minikube | pradyuman | v1.36.0 | 22 Aug 25 12:09 IST | 22 Aug 25 12:10 IST |
|         | --memory=4000mb                |          |           |         |                     |                     |
| ip      |                                | minikube | pradyuman | v1.36.0 | 22 Aug 25 12:25 IST | 22 Aug 25 12:25 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 22 Aug 25 12:36 IST | 22 Aug 25 12:36 IST |
| service | awx-service -n awx             | minikube | pradyuman | v1.36.0 | 22 Aug 25 12:38 IST | 22 Aug 25 13:23 IST |
| service | awx-service -n awx             | minikube | pradyuman | v1.36.0 | 22 Aug 25 13:48 IST | 22 Aug 25 14:37 IST |
| service | awx-service -n awx             | minikube | pradyuman | v1.36.0 | 22 Aug 25 14:37 IST |                     |
| start   | --driver=docker                | minikube | pradyuman | v1.36.0 | 23 Aug 25 22:23 IST | 23 Aug 25 22:23 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 23 Aug 25 23:49 IST | 23 Aug 25 23:49 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:00 IST | 24 Aug 25 00:04 IST |
| ip      |                                | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:03 IST | 24 Aug 25 00:03 IST |
| tunnel  |                                | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:07 IST |                     |
| service | myapp                          | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:09 IST |                     |
| service | myapp                          | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:11 IST |                     |
| service | myapp --url                    | minikube | pradyuman | v1.36.0 | 24 Aug 25 00:17 IST |                     |
|---------|--------------------------------|----------|-----------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/08/23 22:23:36
Running on machine: Pradyumans-Mac-mini
Binary: Built with gc go1.24.0 for darwin/arm64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0823 22:23:36.916120    8064 out.go:345] Setting OutFile to fd 1 ...
I0823 22:23:36.916441    8064 out.go:397] isatty.IsTerminal(1) = true
I0823 22:23:36.916443    8064 out.go:358] Setting ErrFile to fd 2...
I0823 22:23:36.916444    8064 out.go:397] isatty.IsTerminal(2) = true
I0823 22:23:36.916769    8064 root.go:338] Updating PATH: /Users/pradyuman/.minikube/bin
I0823 22:23:36.918207    8064 out.go:352] Setting JSON to false
I0823 22:23:36.938352    8064 start.go:130] hostinfo: {"hostname":"Pradyumans-Mac-mini.local","uptime":2647,"bootTime":1755965369,"procs":688,"os":"darwin","platform":"darwin","platformFamily":"Standalone Workstation","platformVersion":"15.4.1","kernelVersion":"24.4.0","kernelArch":"arm64","virtualizationSystem":"","virtualizationRole":"","hostId":"c920c699-e523-5c80-898a-3971cafcfb9a"}
W0823 22:23:36.938409    8064 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0823 22:23:36.946723    8064 out.go:177] 😄  minikube v1.36.0 on Darwin 15.4.1 (arm64)
I0823 22:23:36.959089    8064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0823 22:23:36.959600    8064 notify.go:220] Checking for updates...
I0823 22:23:36.961010    8064 driver.go:404] Setting default libvirt URI to qemu:///system
I0823 22:23:36.982354    8064 docker.go:123] docker version: linux-28.3.2:Docker Desktop 4.43.2 (199162)
I0823 22:23:36.982813    8064 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0823 22:23:37.425844    8064 info.go:266] docker info: {ID:63f056e2-ee7d-46b1-92c2-380d5ae60b94 Containers:7 ContainersRunning:2 ContainersPaused:0 ContainersStopped:5 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:68 OomKillDisable:false NGoroutines:104 SystemTime:2025-08-23 16:53:37.413139636 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8218034176 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/pradyuman/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/pradyuman/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/Users/pradyuman/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/Users/pradyuman/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/Users/pradyuman/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/Users/pradyuman/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/Users/pradyuman/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/Users/pradyuman/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/Users/pradyuman/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/pradyuman/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/Users/pradyuman/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/Users/pradyuman/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/pradyuman/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I0823 22:23:37.430706    8064 out.go:177] ✨  Using the docker driver based on existing profile
I0823 22:23:37.438690    8064 start.go:304] selected driver: docker
I0823 22:23:37.438705    8064 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0823 22:23:37.438727    8064 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0823 22:23:37.438809    8064 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0823 22:23:37.517091    8064 info.go:266] docker info: {ID:63f056e2-ee7d-46b1-92c2-380d5ae60b94 Containers:7 ContainersRunning:2 ContainersPaused:0 ContainersStopped:5 Images:8 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:68 OomKillDisable:false NGoroutines:104 SystemTime:2025-08-23 16:53:37.50820272 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:15 KernelVersion:6.10.14-linuxkit OperatingSystem:Docker Desktop OSType:linux Architecture:aarch64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:10 MemTotal:8218034176 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=unix:///Users/pradyuman/Library/Containers/com.docker.docker/Data/docker-cli.sock] ExperimentalBuild:false ServerVersion:28.3.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:[WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:/Users/pradyuman/.docker/cli-plugins/docker-ai SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.6.0] map[Name:buildx Path:/Users/pradyuman/.docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.25.0-desktop.1] map[Name:cloud Path:/Users/pradyuman/.docker/cli-plugins/docker-cloud SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.4.2] map[Name:compose Path:/Users/pradyuman/.docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.38.2-desktop.1] map[Name:debug Path:/Users/pradyuman/.docker/cli-plugins/docker-debug SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.41] map[Name:desktop Path:/Users/pradyuman/.docker/cli-plugins/docker-desktop SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.11] map[Name:extension Path:/Users/pradyuman/.docker/cli-plugins/docker-extension SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.29] map[Name:init Path:/Users/pradyuman/.docker/cli-plugins/docker-init SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:/Users/pradyuman/.docker/cli-plugins/docker-mcp SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:v0.9.9] map[Name:model Path:/Users/pradyuman/.docker/cli-plugins/docker-model SchemaVersion:0.1.0 ShortDescription:Docker Model Runner (EXPERIMENTAL) Vendor:Docker Inc. Version:v0.1.33] map[Name:sbom Path:/Users/pradyuman/.docker/cli-plugins/docker-sbom SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:/Users/pradyuman/.docker/cli-plugins/docker-scout SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.18.1]] Warnings:<nil>}}
I0823 22:23:37.518044    8064 cni.go:84] Creating CNI manager for ""
I0823 22:23:37.518089    8064 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0823 22:23:37.518131    8064 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0823 22:23:37.527242    8064 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0823 22:23:37.531296    8064 cache.go:121] Beginning downloading kic base image for docker with docker
I0823 22:23:37.535245    8064 out.go:177] 🚜  Pulling base image v0.0.47 ...
I0823 22:23:37.545549    8064 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0823 22:23:37.545564    8064 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0823 22:23:37.545617    8064 preload.go:146] Found local preload: /Users/pradyuman/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4
I0823 22:23:37.545634    8064 cache.go:56] Caching tarball of preloaded images
I0823 22:23:37.545872    8064 preload.go:172] Found /Users/pradyuman/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.33.1-docker-overlay2-arm64.tar.lz4 in cache, skipping download
I0823 22:23:37.545884    8064 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0823 22:23:37.546098    8064 profile.go:143] Saving config to /Users/pradyuman/.minikube/profiles/minikube/config.json ...
I0823 22:23:37.606724    8064 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0823 22:23:37.606996    8064 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0823 22:23:37.607029    8064 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0823 22:23:37.607036    8064 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0823 22:23:37.607040    8064 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0823 22:23:37.607042    8064 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0823 22:23:44.394521    8064 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0823 22:23:44.394609    8064 cache.go:230] Successfully downloaded all kic artifacts
I0823 22:23:44.395460    8064 start.go:360] acquireMachinesLock for minikube: {Name:mkdfdda14780997b811e0897826f01f99de6f94b Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0823 22:23:44.395756    8064 start.go:364] duration metric: took 273.667µs to acquireMachinesLock for "minikube"
I0823 22:23:44.395791    8064 start.go:96] Skipping create...Using existing machine configuration
I0823 22:23:44.395795    8064 fix.go:54] fixHost starting: 
I0823 22:23:44.396085    8064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0823 22:23:44.416176    8064 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0823 22:23:44.416201    8064 fix.go:138] unexpected machine state, will restart: <nil>
I0823 22:23:44.420886    8064 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0823 22:23:44.429144    8064 cli_runner.go:164] Run: docker start minikube
I0823 22:23:44.534099    8064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0823 22:23:44.553452    8064 kic.go:430] container "minikube" state is running.
I0823 22:23:44.553808    8064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0823 22:23:44.567059    8064 profile.go:143] Saving config to /Users/pradyuman/.minikube/profiles/minikube/config.json ...
I0823 22:23:44.567302    8064 machine.go:93] provisionDockerMachine start ...
I0823 22:23:44.567348    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:44.580700    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:44.581287    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:44.581291    8064 main.go:141] libmachine: About to run SSH command:
hostname
I0823 22:23:44.582261    8064 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0823 22:23:47.712832    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0823 22:23:47.713271    8064 ubuntu.go:169] provisioning hostname "minikube"
I0823 22:23:47.713644    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:47.749817    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:47.750148    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:47.750154    8064 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0823 22:23:47.875163    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0823 22:23:47.875342    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:47.906672    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:47.906900    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:47.906908    8064 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0823 22:23:48.011441    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0823 22:23:48.011460    8064 ubuntu.go:175] set auth options {CertDir:/Users/pradyuman/.minikube CaCertPath:/Users/pradyuman/.minikube/certs/ca.pem CaPrivateKeyPath:/Users/pradyuman/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/Users/pradyuman/.minikube/machines/server.pem ServerKeyPath:/Users/pradyuman/.minikube/machines/server-key.pem ClientKeyPath:/Users/pradyuman/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/Users/pradyuman/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/Users/pradyuman/.minikube}
I0823 22:23:48.011496    8064 ubuntu.go:177] setting up certificates
I0823 22:23:48.014368    8064 provision.go:84] configureAuth start
I0823 22:23:48.014523    8064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0823 22:23:48.044211    8064 provision.go:143] copyHostCerts
I0823 22:23:48.045331    8064 exec_runner.go:144] found /Users/pradyuman/.minikube/ca.pem, removing ...
I0823 22:23:48.045539    8064 exec_runner.go:203] rm: /Users/pradyuman/.minikube/ca.pem
I0823 22:23:48.045709    8064 exec_runner.go:151] cp: /Users/pradyuman/.minikube/certs/ca.pem --> /Users/pradyuman/.minikube/ca.pem (1086 bytes)
I0823 22:23:48.046108    8064 exec_runner.go:144] found /Users/pradyuman/.minikube/cert.pem, removing ...
I0823 22:23:48.046112    8064 exec_runner.go:203] rm: /Users/pradyuman/.minikube/cert.pem
I0823 22:23:48.046181    8064 exec_runner.go:151] cp: /Users/pradyuman/.minikube/certs/cert.pem --> /Users/pradyuman/.minikube/cert.pem (1127 bytes)
I0823 22:23:48.046536    8064 exec_runner.go:144] found /Users/pradyuman/.minikube/key.pem, removing ...
I0823 22:23:48.046542    8064 exec_runner.go:203] rm: /Users/pradyuman/.minikube/key.pem
I0823 22:23:48.046741    8064 exec_runner.go:151] cp: /Users/pradyuman/.minikube/certs/key.pem --> /Users/pradyuman/.minikube/key.pem (1679 bytes)
I0823 22:23:48.047034    8064 provision.go:117] generating server cert: /Users/pradyuman/.minikube/machines/server.pem ca-key=/Users/pradyuman/.minikube/certs/ca.pem private-key=/Users/pradyuman/.minikube/certs/ca-key.pem org=pradyuman.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0823 22:23:48.309400    8064 provision.go:177] copyRemoteCerts
I0823 22:23:48.310559    8064 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0823 22:23:48.310612    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.327504    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:48.412531    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0823 22:23:48.430823    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1086 bytes)
I0823 22:23:48.442705    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/machines/server.pem --> /etc/docker/server.pem (1188 bytes)
I0823 22:23:48.452596    8064 provision.go:87] duration metric: took 438.219875ms to configureAuth
I0823 22:23:48.452602    8064 ubuntu.go:193] setting minikube options for container-runtime
I0823 22:23:48.452719    8064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0823 22:23:48.452763    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.469902    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:48.470089    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:48.470093    8064 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0823 22:23:48.568705    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0823 22:23:48.568713    8064 ubuntu.go:71] root file system type: overlay
I0823 22:23:48.568784    8064 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0823 22:23:48.568883    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.598574    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:48.598811    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:48.598849    8064 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0823 22:23:48.707178    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0823 22:23:48.707288    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.737241    8064 main.go:141] libmachine: Using SSH client type: native
I0823 22:23:48.737496    8064 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x1033dbe40] 0x1033de600 <nil>  [] 0s} 127.0.0.1 50262 <nil> <nil>}
I0823 22:23:48.737507    8064 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0823 22:23:48.846792    8064 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0823 22:23:48.846829    8064 machine.go:96] duration metric: took 4.279491625s to provisionDockerMachine
I0823 22:23:48.846838    8064 start.go:293] postStartSetup for "minikube" (driver="docker")
I0823 22:23:48.846846    8064 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0823 22:23:48.846964    8064 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0823 22:23:48.847029    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.877145    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:48.957854    8064 ssh_runner.go:195] Run: cat /etc/os-release
I0823 22:23:48.960202    8064 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0823 22:23:48.960248    8064 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0823 22:23:48.960262    8064 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0823 22:23:48.960273    8064 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0823 22:23:48.960286    8064 filesync.go:126] Scanning /Users/pradyuman/.minikube/addons for local assets ...
I0823 22:23:48.960556    8064 filesync.go:126] Scanning /Users/pradyuman/.minikube/files for local assets ...
I0823 22:23:48.960661    8064 start.go:296] duration metric: took 113.813625ms for postStartSetup
I0823 22:23:48.960840    8064 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0823 22:23:48.960950    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:48.995248    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:49.077933    8064 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0823 22:23:49.080914    8064 fix.go:56] duration metric: took 4.685082875s for fixHost
I0823 22:23:49.080930    8064 start.go:83] releasing machines lock for "minikube", held for 4.685134208s
I0823 22:23:49.081072    8064 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0823 22:23:49.115633    8064 ssh_runner.go:195] Run: cat /version.json
I0823 22:23:49.115724    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:49.116182    8064 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0823 22:23:49.116761    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:49.136234    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:49.136668    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:49.463102    8064 ssh_runner.go:195] Run: systemctl --version
I0823 22:23:49.469067    8064 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0823 22:23:49.471822    8064 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0823 22:23:49.482319    8064 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0823 22:23:49.482449    8064 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0823 22:23:49.486494    8064 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0823 22:23:49.486512    8064 start.go:495] detecting cgroup driver to use...
I0823 22:23:49.486836    8064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0823 22:23:49.487950    8064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0823 22:23:49.495114    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0823 22:23:49.499361    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0823 22:23:49.503492    8064 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0823 22:23:49.503527    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0823 22:23:49.506982    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0823 22:23:49.510322    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0823 22:23:49.513635    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0823 22:23:49.516750    8064 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0823 22:23:49.519678    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0823 22:23:49.522747    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0823 22:23:49.525868    8064 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0823 22:23:49.528931    8064 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0823 22:23:49.531982    8064 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0823 22:23:49.534486    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:49.558256    8064 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0823 22:23:49.603909    8064 start.go:495] detecting cgroup driver to use...
I0823 22:23:49.603925    8064 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0823 22:23:49.604024    8064 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0823 22:23:49.608725    8064 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0823 22:23:49.608807    8064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0823 22:23:49.613291    8064 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0823 22:23:49.619058    8064 ssh_runner.go:195] Run: which cri-dockerd
I0823 22:23:49.620381    8064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0823 22:23:49.623078    8064 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0823 22:23:49.628463    8064 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0823 22:23:49.651236    8064 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0823 22:23:49.677159    8064 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0823 22:23:49.677236    8064 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0823 22:23:49.684305    8064 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0823 22:23:49.688605    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:49.738314    8064 ssh_runner.go:195] Run: sudo systemctl restart docker
I0823 22:23:51.444316    8064 ssh_runner.go:235] Completed: sudo systemctl restart docker: (1.705972917s)
I0823 22:23:51.444422    8064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0823 22:23:51.449158    8064 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0823 22:23:51.453976    8064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0823 22:23:51.457731    8064 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0823 22:23:51.482338    8064 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0823 22:23:51.505801    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:51.528119    8064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0823 22:23:51.543924    8064 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0823 22:23:51.547356    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:51.570738    8064 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0823 22:23:51.702206    8064 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0823 22:23:51.706152    8064 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0823 22:23:51.706523    8064 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0823 22:23:51.707763    8064 start.go:563] Will wait 60s for crictl version
I0823 22:23:51.707808    8064 ssh_runner.go:195] Run: which crictl
I0823 22:23:51.709050    8064 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0823 22:23:51.768543    8064 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0823 22:23:51.768673    8064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0823 22:23:51.819976    8064 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0823 22:23:51.843988    8064 out.go:235] 🐳  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0823 22:23:51.844575    8064 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0823 22:23:51.932217    8064 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0823 22:23:51.932605    8064 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0823 22:23:51.934123    8064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0823 22:23:51.937819    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0823 22:23:51.957825    8064 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0823 22:23:51.957886    8064 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0823 22:23:51.957926    8064 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0823 22:23:51.965740    8064 docker.go:702] Got preloaded images: -- stdout --
quay.io/sclorg/postgresql-15-c9s:latest
venkatesh1409/threadedtreasures:0.0.1
redis:7
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
quay.io/ansible/awx-ee:latest
registry.k8s.io/etcd:3.5.21-0
mongo:5.0
registry.k8s.io/coredns/coredns:v1.12.0
quay.io/ansible/awx-operator:2.19.1
quay.io/ansible/awx:24.6.1
quay.io/ansible/awx-ee:24.6.1
registry.k8s.io/pause:3.10
gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0823 22:23:51.965750    8064 docker.go:632] Images already preloaded, skipping extraction
I0823 22:23:51.966002    8064 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0823 22:23:51.973730    8064 docker.go:702] Got preloaded images: -- stdout --
quay.io/sclorg/postgresql-15-c9s:latest
venkatesh1409/threadedtreasures:0.0.1
redis:7
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
quay.io/ansible/awx-ee:latest
registry.k8s.io/etcd:3.5.21-0
mongo:5.0
registry.k8s.io/coredns/coredns:v1.12.0
quay.io/ansible/awx-operator:2.19.1
quay.io/ansible/awx:24.6.1
quay.io/ansible/awx-ee:24.6.1
registry.k8s.io/pause:3.10
gcr.io/kubebuilder/kube-rbac-proxy:v0.15.0
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0823 22:23:51.973734    8064 cache_images.go:84] Images are preloaded, skipping loading
I0823 22:23:51.973752    8064 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0823 22:23:51.974017    8064 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0823 22:23:51.974057    8064 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0823 22:23:52.075689    8064 cni.go:84] Creating CNI manager for ""
I0823 22:23:52.075702    8064 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0823 22:23:52.075942    8064 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0823 22:23:52.075968    8064 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0823 22:23:52.076088    8064 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0823 22:23:52.076212    8064 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0823 22:23:52.079743    8064 binaries.go:44] Found k8s binaries, skipping transfer
I0823 22:23:52.079825    8064 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0823 22:23:52.082476    8064 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0823 22:23:52.087660    8064 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0823 22:23:52.092751    8064 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0823 22:23:52.097944    8064 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0823 22:23:52.099124    8064 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0823 22:23:52.102363    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:52.121813    8064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0823 22:23:52.144671    8064 certs.go:68] Setting up /Users/pradyuman/.minikube/profiles/minikube for IP: 192.168.49.2
I0823 22:23:52.144676    8064 certs.go:194] generating shared ca certs ...
I0823 22:23:52.144684    8064 certs.go:226] acquiring lock for ca certs: {Name:mk58ddc2435ec355fd18d60565f7451c9aa95b36 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0823 22:23:52.145351    8064 certs.go:235] skipping valid "minikubeCA" ca cert: /Users/pradyuman/.minikube/ca.key
I0823 22:23:52.145718    8064 certs.go:235] skipping valid "proxyClientCA" ca cert: /Users/pradyuman/.minikube/proxy-client-ca.key
I0823 22:23:52.145738    8064 certs.go:256] generating profile certs ...
I0823 22:23:52.146206    8064 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /Users/pradyuman/.minikube/profiles/minikube/client.key
I0823 22:23:52.146452    8064 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /Users/pradyuman/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0823 22:23:52.146676    8064 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /Users/pradyuman/.minikube/profiles/minikube/proxy-client.key
I0823 22:23:52.146973    8064 certs.go:484] found cert: /Users/pradyuman/.minikube/certs/ca-key.pem (1679 bytes)
I0823 22:23:52.147037    8064 certs.go:484] found cert: /Users/pradyuman/.minikube/certs/ca.pem (1086 bytes)
I0823 22:23:52.147093    8064 certs.go:484] found cert: /Users/pradyuman/.minikube/certs/cert.pem (1127 bytes)
I0823 22:23:52.147151    8064 certs.go:484] found cert: /Users/pradyuman/.minikube/certs/key.pem (1679 bytes)
I0823 22:23:52.148158    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0823 22:23:52.155761    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0823 22:23:52.163180    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0823 22:23:52.170507    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0823 22:23:52.177689    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0823 22:23:52.185246    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0823 22:23:52.192311    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0823 22:23:52.199731    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0823 22:23:52.207233    8064 ssh_runner.go:362] scp /Users/pradyuman/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0823 22:23:52.214896    8064 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0823 22:23:52.220840    8064 ssh_runner.go:195] Run: openssl version
I0823 22:23:52.225272    8064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0823 22:23:52.229157    8064 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0823 22:23:52.230293    8064 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan 25  2025 /usr/share/ca-certificates/minikubeCA.pem
I0823 22:23:52.230320    8064 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0823 22:23:52.232668    8064 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0823 22:23:52.235591    8064 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0823 22:23:52.236799    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0823 22:23:52.240484    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0823 22:23:52.244417    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0823 22:23:52.246782    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0823 22:23:52.249202    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0823 22:23:52.251776    8064 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0823 22:23:52.254297    8064 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/Users:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0823 22:23:52.254367    8064 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0823 22:23:52.263273    8064 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0823 22:23:52.266478    8064 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0823 22:23:52.266717    8064 kubeadm.go:589] restartPrimaryControlPlane start ...
I0823 22:23:52.266788    8064 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0823 22:23:52.270033    8064 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0823 22:23:52.270144    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0823 22:23:52.293984    8064 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:49803"
I0823 22:23:52.294010    8064 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:49803, want: 127.0.0.1:50264
I0823 22:23:52.294274    8064 kubeconfig.go:62] /Users/pradyuman/.kube/config needs updating (will repair): [kubeconfig needs server address update]
I0823 22:23:52.295056    8064 lock.go:35] WriteFile acquiring /Users/pradyuman/.kube/config: {Name:mk574bf9ee0b621b6bf59953f3712b1502c70c00 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0823 22:23:52.303286    8064 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0823 22:23:52.308004    8064 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0823 22:23:52.308026    8064 kubeadm.go:593] duration metric: took 41.300416ms to restartPrimaryControlPlane
I0823 22:23:52.308031    8064 kubeadm.go:394] duration metric: took 53.745292ms to StartCluster
I0823 22:23:52.308053    8064 settings.go:142] acquiring lock: {Name:mk8dc483b04092acbfa2915d2592c699101b0630 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0823 22:23:52.308206    8064 settings.go:150] Updating kubeconfig:  /Users/pradyuman/.kube/config
I0823 22:23:52.308864    8064 lock.go:35] WriteFile acquiring /Users/pradyuman/.kube/config: {Name:mk574bf9ee0b621b6bf59953f3712b1502c70c00 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0823 22:23:52.309211    8064 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0823 22:23:52.309323    8064 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0823 22:23:52.309379    8064 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0823 22:23:52.309415    8064 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0823 22:23:52.309419    8064 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0823 22:23:52.309421    8064 addons.go:247] addon storage-provisioner should already be in state true
I0823 22:23:52.309429    8064 host.go:66] Checking if "minikube" exists ...
I0823 22:23:52.309427    8064 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0823 22:23:52.309434    8064 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0823 22:23:52.309803    8064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0823 22:23:52.309945    8064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0823 22:23:52.319241    8064 out.go:177] 🔎  Verifying Kubernetes components...
I0823 22:23:52.323336    8064 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0823 22:23:52.331537    8064 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0823 22:23:52.331543    8064 addons.go:247] addon default-storageclass should already be in state true
I0823 22:23:52.331552    8064 host.go:66] Checking if "minikube" exists ...
I0823 22:23:52.331716    8064 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0823 22:23:52.333754    8064 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0823 22:23:52.339917    8064 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0823 22:23:52.339929    8064 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0823 22:23:52.340042    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:52.350064    8064 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0823 22:23:52.350074    8064 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0823 22:23:52.350144    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0823 22:23:52.356799    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:52.361693    8064 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0823 22:23:52.365746    8064 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:50262 SSHKeyPath:/Users/pradyuman/.minikube/machines/minikube/id_rsa Username:docker}
I0823 22:23:52.368642    8064 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0823 22:23:52.384108    8064 api_server.go:52] waiting for apiserver process to appear ...
I0823 22:23:52.384202    8064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0823 22:23:52.435401    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0823 22:23:52.472780    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
W0823 22:23:52.598042    8064 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.598073    8064 retry.go:31] will retry after 134.153982ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0823 22:23:52.599299    8064 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.599309    8064 retry.go:31] will retry after 145.813263ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.733428    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0823 22:23:52.746212    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0823 22:23:52.774787    8064 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.774813    8064 retry.go:31] will retry after 343.366118ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0823 22:23:52.778765    8064 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.778784    8064 retry.go:31] will retry after 460.166928ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:52.884553    8064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0823 22:23:53.119372    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0823 22:23:53.149437    8064 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:53.149449    8064 retry.go:31] will retry after 638.244427ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0823 22:23:53.239691    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0823 22:23:53.385383    8064 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0823 22:23:53.788955    8064 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0823 22:23:54.239715    8064 api_server.go:72] duration metric: took 1.930478458s to wait for apiserver process to appear ...
I0823 22:23:54.239725    8064 api_server.go:88] waiting for apiserver healthz status ...
I0823 22:23:54.239739    8064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50264/healthz ...
I0823 22:23:54.248191    8064 api_server.go:279] https://127.0.0.1:50264/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0823 22:23:54.248203    8064 api_server.go:103] status: https://127.0.0.1:50264/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0823 22:23:54.494520    8064 out.go:177] 🌟  Enabled addons: default-storageclass, storage-provisioner
I0823 22:23:54.498642    8064 addons.go:514] duration metric: took 2.189399333s for enable addons: enabled=[default-storageclass storage-provisioner]
I0823 22:23:54.740430    8064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50264/healthz ...
I0823 22:23:54.748063    8064 api_server.go:279] https://127.0.0.1:50264/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0823 22:23:54.748085    8064 api_server.go:103] status: https://127.0.0.1:50264/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0823 22:23:55.240840    8064 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:50264/healthz ...
I0823 22:23:55.245903    8064 api_server.go:279] https://127.0.0.1:50264/healthz returned 200:
ok
I0823 22:23:55.246652    8064 api_server.go:141] control plane version: v1.33.1
I0823 22:23:55.246663    8064 api_server.go:131] duration metric: took 1.006927208s to wait for apiserver health ...
I0823 22:23:55.247536    8064 system_pods.go:43] waiting for kube-system pods to appear ...
I0823 22:23:55.254373    8064 system_pods.go:59] 7 kube-system pods found
I0823 22:23:55.254387    8064 system_pods.go:61] "coredns-674b8bbfcf-jb55t" [991fc1ae-b2e2-4d97-8075-9c187c3fd166] Running
I0823 22:23:55.254391    8064 system_pods.go:61] "etcd-minikube" [19f596fe-8e6a-4407-86e6-f408ade8b68a] Running
I0823 22:23:55.254393    8064 system_pods.go:61] "kube-apiserver-minikube" [396858fc-f2b5-4704-828e-43b75e84a789] Running
I0823 22:23:55.254395    8064 system_pods.go:61] "kube-controller-manager-minikube" [f5c6f022-135f-4e56-bb5f-0aef4a634c44] Running
I0823 22:23:55.254426    8064 system_pods.go:61] "kube-proxy-p5z4x" [b389e27f-fa60-415b-8d4c-873fdc875823] Running
I0823 22:23:55.254431    8064 system_pods.go:61] "kube-scheduler-minikube" [81ff51bd-56b9-4e72-9494-e386e5bf63a1] Running
I0823 22:23:55.254433    8064 system_pods.go:61] "storage-provisioner" [d4e9f929-1c9c-4f31-9301-27c754106940] Running
I0823 22:23:55.254437    8064 system_pods.go:74] duration metric: took 6.895375ms to wait for pod list to return data ...
I0823 22:23:55.254444    8064 kubeadm.go:578] duration metric: took 2.945203833s to wait for: map[apiserver:true system_pods:true]
I0823 22:23:55.254451    8064 node_conditions.go:102] verifying NodePressure condition ...
I0823 22:23:55.255571    8064 node_conditions.go:122] node storage ephemeral capacity is 1055761844Ki
I0823 22:23:55.255578    8064 node_conditions.go:123] node cpu capacity is 10
I0823 22:23:55.255798    8064 node_conditions.go:105] duration metric: took 1.344ms to run NodePressure ...
I0823 22:23:55.255805    8064 start.go:241] waiting for startup goroutines ...
I0823 22:23:55.255809    8064 start.go:246] waiting for cluster config update ...
I0823 22:23:55.255817    8064 start.go:255] writing updated cluster config ...
I0823 22:23:55.256396    8064 ssh_runner.go:195] Run: rm -f paused
I0823 22:23:55.456837    8064 start.go:607] kubectl: 1.33.2, cluster: 1.33.1 (minor skew: 0)
I0823 22:23:55.462081    8064 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Aug 23 16:53:51 minikube cri-dockerd[1439]: time="2025-08-23T16:53:51Z" level=info msg="Start cri-dockerd grpc backend"
Aug 23 16:53:51 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-5r8jr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9e049ca853199dd83304e4be25b41f215d1202eb6e921f220abab85a57f8bdc4\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-5r8jr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ca7d7101ced7527ab9ea205b9363b57d2f951f36839144d5f7e70ef4ada6298b\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2f8m8_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8c8ecce869889b3cc8e41de53b81a5260bce82948229f891aa728d55cc2945fb\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2f8m8_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"04c84f90f347c7c9522be917dfcf3f1c2f904aa56f92c0cd2fdc51a8a3adb3c1\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-postgres-15-0_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f7d80baea6ce876e73c6931e694b8a1b019dbad5ff99ff9505a32f5a971e6c92\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-operator-controller-manager-799967c78c-b7frh_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c42f75c914b35dc740b9fa1b2fec21d27f72aba9d9689eda19cec82c3c5a0497\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-jb55t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"431e470652b93aeeaccfa5c59f00c107c8c2016e0cba48396255af0ba347318f\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-zdpqc_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f3b2eafc435ef9f6054339bf53806f561c2a5df2510ac6989df4c648871aa27b\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-zdpqc_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0f1341f54da9336331db28248dc1044c674d204dc6d8717ff5a3f77906def9d3\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-task-67fb8f55c5-ljf84_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"72fcb3de72c782564bd4a97b7f8e58b5d073c8e80278b2e80c030b840bab6fe2\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-web-5dd647cf67-nhwrb_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"2b09fe25bad20a45ac87b0cb5bf3f8bad41d61ece1644ba6a29e896c241a8c62\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-xh874_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"49dc6688bd4f2298937762530d77edd90a6673a218f8c7dca9dbd663da1aa509\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-xh874_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"800c070fae11d3f89b701317a680e7f3ac2d766a6c35439617e7240867638da8\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2rd6s_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"577f4a9513a19fde73d2d9c8f657fade100654ccdf79bfcadc83d9280e9ea838\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2rd6s_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"493cca248a773bd43e0880ef9598d4028850aab125943acb27cb905f90f0d3b4\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-migration-24.6.1-hh77g_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"96a41caea2eeca8a69b6eb75768938b3b2657076ead68734d01bcc757a13da69\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-lmbp2_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9c0993eddba0aeccbf1e7a7dd207e0750c1444ef3bedd7d9f665c79b1e78a2da\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-lmbp2_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"7a895f9e178cc4d1c6a8898f3a7d24e658e07770b0a8f251d136b488a048b790\""
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"b3dc3858392ccc37368631ad10a519ccda77d840f79bb4884f723b62a77b4c98\". Proceed without further sandbox information."
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"69716cfbfa6ff32fc0f866aeba0f2ac4816ca5228c4e7a4bcad33a7e2a3fe7fa\". Proceed without further sandbox information."
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"e55b3c56a07dd49a03d48cacee7c88135640d3f374d363a28a3240fdcde5325b\". Proceed without further sandbox information."
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ba16f5b2ed0ddf565b19a24f8545f1fe2c4d39ec426b0b66a35c6f951ba2d51/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/152f3bfdc1e8b7173dab55a6c615ad77d3e80e3ed47e550aa99a12863f97d8c9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f1ec1941eb7cc7a33003089b7c62d6e9acfa8d6191b56691a25464d9d7b7ccc9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:52 minikube cri-dockerd[1439]: time="2025-08-23T16:53:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/edc983041c449de7427fe19cf94aae86b7ab971b5fae3e037a449298617eb58c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-5r8jr_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9e049ca853199dd83304e4be25b41f215d1202eb6e921f220abab85a57f8bdc4\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2f8m8_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"8c8ecce869889b3cc8e41de53b81a5260bce82948229f891aa728d55cc2945fb\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"awx-operator-controller-manager-799967c78c-b7frh_awx\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"c42f75c914b35dc740b9fa1b2fec21d27f72aba9d9689eda19cec82c3c5a0497\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-jb55t_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"431e470652b93aeeaccfa5c59f00c107c8c2016e0cba48396255af0ba347318f\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-zdpqc_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"f3b2eafc435ef9f6054339bf53806f561c2a5df2510ac6989df4c648871aa27b\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"webapp-deployment-5d89486896-xh874_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"49dc6688bd4f2298937762530d77edd90a6673a218f8c7dca9dbd663da1aa509\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-2rd6s_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"577f4a9513a19fde73d2d9c8f657fade100654ccdf79bfcadc83d9280e9ea838\""
Aug 23 16:53:53 minikube cri-dockerd[1439]: time="2025-08-23T16:53:53Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"mongo-deployment-5fbf68db69-lmbp2_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9c0993eddba0aeccbf1e7a7dd207e0750c1444ef3bedd7d9f665c79b1e78a2da\""
Aug 23 16:53:54 minikube cri-dockerd[1439]: time="2025-08-23T16:53:54Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Aug 23 16:53:56 minikube cri-dockerd[1439]: time="2025-08-23T16:53:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1ee571c527ae67d5ddf9187548cf68ebf16f1f2e431a87cffbaea3688031e92c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:56 minikube cri-dockerd[1439]: time="2025-08-23T16:53:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7609134b192514818e468251cc3d001c16556a78595723009d0348637462fa2a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:56 minikube cri-dockerd[1439]: time="2025-08-23T16:53:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/51be89c92f8f1d6b26972bf7b49dd3aa64cb4a21a1efdd02da989c6ee7068a64/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Aug 23 16:53:56 minikube cri-dockerd[1439]: time="2025-08-23T16:53:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1c697fb783e617dcf26b6f82daab514f6d4229d82eb88dfc546b925b060fb04f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/dd10e165da829e50cb25c713719914082cc29149ff3901fe0d2611e4d2558945/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c45a687204cc32022de6e1e3949c7978633370a12844ec2cae5af160f535207f/resolv.conf as [nameserver 10.96.0.10 search awx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21c3b291e4e4e7763b658f036986463a71d00e6a970062d4eb4d2091c708d686/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f25400a9a1ea87bda28f19092c93b9791602d69ac98089e4702e6a452015588c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/aa6fbe3790c6fea26947dd69b3211fd3c8039475aa3a80e677a6464413217485/resolv.conf as [nameserver 10.96.0.10 search awx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b22c6c0712e9047d2a5b8a18efd21e02c3bc2e5655a552059dcbb6b309f2faa2/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/830beb841a2935194ae799feaa368036739a307860391e2e98918f741c98f430/resolv.conf as [nameserver 10.96.0.10 search awx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c250b6010451fc06eb95dd460e6eba898b5403473fec955d11bc26fbfdfc97cd/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:53:57 minikube cri-dockerd[1439]: time="2025-08-23T16:53:57Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c6251ceb21a5fba90cf21e029c8eb606349bcd169c29cd8b6b80e11658185ae1/resolv.conf as [nameserver 10.96.0.10 search awx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Aug 23 16:54:27 minikube dockerd[1092]: time="2025-08-23T16:54:27.765665798Z" level=info msg="ignoring event" container=04348fb9f3b4ea38fac98a97ea55fff2b863ef74a0e9c8be6399223ab2b7d05e module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 23 16:54:44 minikube dockerd[1092]: time="2025-08-23T16:54:44.859308334Z" level=info msg="ignoring event" container=076c9fc12d381b27671ad18c9910769f523e317b6d7258bc8bd4d432d189852d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 23 16:54:45 minikube dockerd[1092]: time="2025-08-23T16:54:45.453439001Z" level=info msg="ignoring event" container=7ee020120afa478a85c2cacbaf6b241fb886246faddccdcc706dd770538e32dd module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Aug 23 16:55:03 minikube cri-dockerd[1439]: E0823 16:55:03.034588    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:40080: use of closed network connection
Aug 23 16:55:03 minikube cri-dockerd[1439]: E0823 16:55:03.537591    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:40088: use of closed network connection
Aug 23 16:55:05 minikube cri-dockerd[1439]: E0823 16:55:05.181626    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:40102: use of closed network connection
Aug 23 16:55:06 minikube cri-dockerd[1439]: E0823 16:55:06.181630    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:41724: use of closed network connection
Aug 23 16:55:07 minikube cri-dockerd[1439]: E0823 16:55:07.194461    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:41730: use of closed network connection
Aug 23 16:55:08 minikube cri-dockerd[1439]: E0823 16:55:08.191584    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:41734: use of closed network connection
Aug 23 16:55:09 minikube cri-dockerd[1439]: E0823 16:55:09.671970    1439 conn.go:315] Error on socket receive: read tcp [::1]:43305->[::1]:41746: use of closed network connection
Aug 23 18:17:04 minikube cri-dockerd[1439]: time="2025-08-23T18:17:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/56ef47770538b6411d32077f70f7dcbce589de0b8729f6ad1193c323333bf79a/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE                                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
0d3dd8636ae8f       e280e6853c4e6                                                                                              30 minutes ago      Running             myapp                     0                   56ef47770538b       myapp-76578df95d-dpw4j
a2038aca37681       cf93f90916940                                                                                              2 hours ago         Running             awx-rsyslog               1                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
715b9dfd550e8       3302e3e3aa9f4                                                                                              2 hours ago         Running             awx-ee                    1                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
1b3ece0198cec       cf93f90916940                                                                                              2 hours ago         Running             awx-task                  1                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
eeec6aeaf0240       08b374128ebd2                                                                                              2 hours ago         Running             redis                     1                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
7ee020120afa4       3302e3e3aa9f4                                                                                              2 hours ago         Exited              init-receptor             0                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
ff8b34f7931ff       ba04bb24b9575                                                                                              2 hours ago         Running             storage-provisioner       21                  1ee571c527ae6       storage-provisioner
73c89f35153d8       cf93f90916940                                                                                              2 hours ago         Running             awx-rsyslog               1                   830beb841a293       awx-web-5dd647cf67-nhwrb
49e86c4fa1414       cf93f90916940                                                                                              2 hours ago         Running             awx-web                   1                   830beb841a293       awx-web-5dd647cf67-nhwrb
afb495a1cf2bd       9018dfbb8d064                                                                                              2 hours ago         Running             mongodb                   2                   1c697fb783e61       mongo-deployment-5fbf68db69-2f8m8
43b9ede0a338a       08b374128ebd2                                                                                              2 hours ago         Running             redis                     1                   830beb841a293       awx-web-5dd647cf67-nhwrb
f32ee8d95ba5a       3e426073a7842                                                                                              2 hours ago         Running             postgres                  1                   c6251ceb21a5f       awx-postgres-15-0
0a4dc70ff3a83       e280e6853c4e6                                                                                              2 hours ago         Running             webapp                    2                   c250b6010451f       webapp-deployment-5d89486896-zdpqc
c9378a13306ff       41e4666fa6822                                                                                              2 hours ago         Running             awx-manager               18                  c45a687204cc3       awx-operator-controller-manager-799967c78c-b7frh
27ff13faec075       e280e6853c4e6                                                                                              2 hours ago         Running             webapp                    2                   b22c6c0712e90       webapp-deployment-5d89486896-5r8jr
20bf39cd5cd97       9018dfbb8d064                                                                                              2 hours ago         Running             mongodb                   2                   21c3b291e4e4e       mongo-deployment-5fbf68db69-2rd6s
e163ba00ef68e       e280e6853c4e6                                                                                              2 hours ago         Running             webapp                    2                   dd10e165da829       webapp-deployment-5d89486896-xh874
d15f3c2e73fde       9018dfbb8d064                                                                                              2 hours ago         Running             mongodb                   2                   f25400a9a1ea8       mongo-deployment-5fbf68db69-lmbp2
1943ad8629ee4       e56d15bd61cf8                                                                                              2 hours ago         Running             kube-rbac-proxy           1                   c45a687204cc3       awx-operator-controller-manager-799967c78c-b7frh
076c9fc12d381       cf93f90916940                                                                                              2 hours ago         Exited              init-database             1                   aa6fbe3790c6f       awx-task-67fb8f55c5-ljf84
d2fa07f28c38a       f72407be9e08c                                                                                              2 hours ago         Running             coredns                   3                   51be89c92f8f1       coredns-674b8bbfcf-jb55t
90c7305a485c1       3e58848989f55                                                                                              2 hours ago         Running             kube-proxy                2                   7609134b19251       kube-proxy-p5z4x
04348fb9f3b4e       ba04bb24b9575                                                                                              2 hours ago         Exited              storage-provisioner       20                  1ee571c527ae6       storage-provisioner
2e8be834ce8bf       31747a36ce712                                                                                              2 hours ago         Running             etcd                      2                   edc983041c449       etcd-minikube
453cf426b4e1a       014094c90caac                                                                                              2 hours ago         Running             kube-scheduler            2                   f1ec1941eb7cc       kube-scheduler-minikube
e07a5107179fb       9a2b7cf4f8540                                                                                              2 hours ago         Running             kube-apiserver            3                   2ba16f5b2ed0d       kube-apiserver-minikube
5159343e96f0b       674996a72aa59                                                                                              2 hours ago         Running             kube-controller-manager   2                   152f3bfdc1e8b       kube-controller-manager-minikube
9ee8af121e9fb       41e4666fa6822                                                                                              34 hours ago        Exited              awx-manager               17                  c42f75c914b35       awx-operator-controller-manager-799967c78c-b7frh
cd8c201fc6107       9a2b7cf4f8540                                                                                              34 hours ago        Exited              kube-apiserver            2                   bc404520e5936       kube-apiserver-minikube
4f6fe886d73d6       f72407be9e08c                                                                                              34 hours ago        Exited              coredns                   2                   431e470652b93       coredns-674b8bbfcf-jb55t
d4b19fb0d1f6e       cf93f90916940                                                                                              36 hours ago        Exited              awx-rsyslog               0                   72fcb3de72c78       awx-task-67fb8f55c5-ljf84
5ba00ea25a4a1       3302e3e3aa9f4                                                                                              36 hours ago        Exited              awx-ee                    0                   72fcb3de72c78       awx-task-67fb8f55c5-ljf84
a7764c73b9236       cf93f90916940                                                                                              36 hours ago        Exited              awx-task                  0                   72fcb3de72c78       awx-task-67fb8f55c5-ljf84
c0c42f1672248       08b374128ebd2                                                                                              36 hours ago        Exited              redis                     0                   72fcb3de72c78       awx-task-67fb8f55c5-ljf84
4a51d0885f4e1       cf93f90916940                                                                                              36 hours ago        Exited              migration-job             0                   96a41caea2eec       awx-migration-24.6.1-hh77g
98aad0ca23bc1       cf93f90916940                                                                                              36 hours ago        Exited              awx-rsyslog               0                   2b09fe25bad20       awx-web-5dd647cf67-nhwrb
4ff7e8e47442d       quay.io/ansible/awx@sha256:5d47e132014ae4c6a026a2e8f19ce7b3cf3c8f313d59e44fdfa95cd9f3669209                36 hours ago        Exited              awx-web                   0                   2b09fe25bad20       awx-web-5dd647cf67-nhwrb
c83403fb6c8fe       redis@sha256:4421962706d7a901cbd435282515704040203d67c653160e3a46a27b79c40e19                              36 hours ago        Exited              redis                     0                   2b09fe25bad20       awx-web-5dd647cf67-nhwrb
1e35c8f6607c5       quay.io/sclorg/postgresql-15-c9s@sha256:476931a294e1a088eda80cb3b1cb1462f606bc5f131c2a7d275666bb1112a8e9   36 hours ago        Exited              postgres                  0                   f7d80baea6ce8       awx-postgres-15-0
843051b97d53e       e56d15bd61cf8                                                                                              36 hours ago        Exited              kube-rbac-proxy           0                   c42f75c914b35       awx-operator-controller-manager-799967c78c-b7frh
901d659796acd       e280e6853c4e6                                                                                              36 hours ago        Exited              webapp                    1                   f3b2eafc435ef       webapp-deployment-5d89486896-zdpqc
c811ad3fc9017       e280e6853c4e6                                                                                              36 hours ago        Exited              webapp                    1                   49dc6688bd4f2       webapp-deployment-5d89486896-xh874
d44234b8481b3       9018dfbb8d064                                                                                              36 hours ago        Exited              mongodb                   1                   9c0993eddba0a       mongo-deployment-5fbf68db69-lmbp2
365567d07b7e6       e280e6853c4e6                                                                                              36 hours ago        Exited              webapp                    1                   9e049ca853199       webapp-deployment-5d89486896-5r8jr
7215d9a084e50       9018dfbb8d064                                                                                              36 hours ago        Exited              mongodb                   1                   8c8ecce869889       mongo-deployment-5fbf68db69-2f8m8
39d768e457f15       9018dfbb8d064                                                                                              36 hours ago        Exited              mongodb                   1                   577f4a9513a19       mongo-deployment-5fbf68db69-2rd6s
1bd5384b3d39a       3e58848989f55                                                                                              36 hours ago        Exited              kube-proxy                1                   43d85b437c21d       kube-proxy-p5z4x
13c4d9508b278       014094c90caac                                                                                              36 hours ago        Exited              kube-scheduler            1                   43a01a8df997b       kube-scheduler-minikube
468b6866ce6e7       31747a36ce712                                                                                              36 hours ago        Exited              etcd                      1                   aff9702825817       etcd-minikube
3b86c986d23b1       674996a72aa59                                                                                              36 hours ago        Exited              kube-controller-manager   1                   2b7b8c70d4d23       kube-controller-manager-minikube


==> coredns [4f6fe886d73d] <==
[INFO] 10.244.0.20:37793 - 29932 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005193708s
[INFO] 10.244.0.20:37793 - 57073 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00596475s
[INFO] 10.244.0.20:48130 - 21988 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.006495709s
[INFO] 10.244.0.20:48130 - 47846 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.006364042s
[INFO] 10.244.0.20:57599 - 9633 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004004208s
[INFO] 10.244.0.20:57599 - 57535 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005106041s
[INFO] 10.244.0.20:51385 - 54613 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004962041s
[INFO] 10.244.0.20:51385 - 59219 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004493166s
[INFO] 10.244.0.20:59871 - 20552 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004486792s
[INFO] 10.244.0.20:59871 - 2637 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004469542s
[INFO] 10.244.0.20:41456 - 10862 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.0039555s
[INFO] 10.244.0.20:41456 - 62800 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004212208s
[INFO] 10.244.0.20:54008 - 1480 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005120917s
[INFO] 10.244.0.20:54008 - 56014 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005928s
[INFO] 10.244.0.20:47853 - 63068 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.002941417s
[INFO] 10.244.0.20:47853 - 46674 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.0033445s
[INFO] 10.244.0.20:42947 - 45954 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004355167s
[INFO] 10.244.0.20:42947 - 54144 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00450025s
[INFO] 10.244.0.20:52111 - 1289 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005255042s
[INFO] 10.244.0.20:52111 - 14087 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004955s
[INFO] 10.244.0.20:54744 - 53419 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.007985666s
[INFO] 10.244.0.20:54744 - 11485 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.007703458s
[INFO] 10.244.0.20:57525 - 29249 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00475275s
[INFO] 10.244.0.20:57525 - 9551 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004644833s
[INFO] 10.244.0.20:44504 - 28000 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005002833s
[INFO] 10.244.0.20:44504 - 30819 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005012583s
[INFO] 10.244.0.20:59470 - 14251 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004644375s
[INFO] 10.244.0.20:59470 - 39862 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.0053545s
[INFO] 10.244.0.20:42793 - 51697 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.006663333s
[INFO] 10.244.0.20:42793 - 54735 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.006660084s
[INFO] 10.244.0.20:58193 - 45454 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.002868292s
[INFO] 10.244.0.20:58193 - 64908 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.003382s
[INFO] 10.244.0.20:40629 - 17182 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005405834s
[INFO] 10.244.0.20:40629 - 37404 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004691083s
[INFO] 10.244.0.20:32788 - 47000 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005429542s
[INFO] 10.244.0.20:32788 - 3461 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005440583s
[INFO] 10.244.0.20:48371 - 57766 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00359825s
[INFO] 10.244.0.20:48371 - 42409 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.003511542s
[INFO] 10.244.0.20:42324 - 29886 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004848875s
[INFO] 10.244.0.20:42324 - 22456 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00552725s
[INFO] 10.244.0.20:54286 - 46768 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.009305292s
[INFO] 10.244.0.20:54286 - 34486 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.0093165s
[INFO] 10.244.0.20:53936 - 51292 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.006712s
[INFO] 10.244.0.20:53936 - 672 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.007059s
[INFO] 10.244.0.20:50162 - 38180 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004953125s
[INFO] 10.244.0.20:50162 - 30502 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.004988041s
[INFO] 10.244.0.20:54018 - 36677 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.005396458s
[INFO] 10.244.0.20:54018 - 37247 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005019459s
[INFO] 10.244.0.20:34889 - 56022 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.003207834s
[INFO] 10.244.0.20:34889 - 64212 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.003597291s
[INFO] 10.244.0.20:56492 - 21000 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.003230416s
[INFO] 10.244.0.20:56492 - 56590 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.004016375s
[INFO] 10.244.0.20:38092 - 41097 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.006903417s
[INFO] 10.244.0.20:38092 - 41607 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00691325s
[INFO] 10.244.0.20:40487 - 2122 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.009157208s
[INFO] 10.244.0.20:40487 - 27702 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.009090792s
[INFO] 10.244.0.20:33831 - 53316 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00372425s
[INFO] 10.244.0.20:33831 - 58178 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00445475s
[INFO] 10.244.0.20:51849 - 6948 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.005025666s
[INFO] 10.244.0.20:51849 - 48418 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00492175s


==> coredns [d2fa07f28c38] <==
[INFO] 10.244.0.29:39457 - 64427 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000260333s
[INFO] 10.244.0.29:39457 - 28072 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000343042s
[INFO] 10.244.0.29:60841 - 25215 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000255916s
[INFO] 10.244.0.29:60841 - 37986 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000338084s
[INFO] 10.244.0.29:53160 - 59241 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00025525s
[INFO] 10.244.0.29:53160 - 63845 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000482917s
[INFO] 10.244.0.29:55607 - 1116 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000244417s
[INFO] 10.244.0.29:55607 - 52049 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000340208s
[INFO] 10.244.0.29:44963 - 65318 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000292709s
[INFO] 10.244.0.29:44963 - 50466 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000380917s
[INFO] 10.244.0.29:38027 - 60757 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000239083s
[INFO] 10.244.0.29:38027 - 16734 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.0003595s
[INFO] 10.244.0.29:48031 - 17294 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000298083s
[INFO] 10.244.0.29:48031 - 6066 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000405917s
[INFO] 10.244.0.29:36935 - 41347 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000256209s
[INFO] 10.244.0.29:36935 - 22972 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00032625s
[INFO] 10.244.0.29:44973 - 21994 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000253959s
[INFO] 10.244.0.29:44973 - 40175 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00034s
[INFO] 10.244.0.29:46412 - 18297 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00025425s
[INFO] 10.244.0.29:46412 - 21117 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000334875s
[INFO] 10.244.0.29:47951 - 60632 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000237417s
[INFO] 10.244.0.29:47951 - 42973 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000339334s
[INFO] 10.244.0.29:34723 - 10845 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000236583s
[INFO] 10.244.0.29:34723 - 58465 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000353917s
[INFO] 10.244.0.29:43507 - 48559 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000249708s
[INFO] 10.244.0.29:43507 - 19380 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000307583s
[INFO] 10.244.0.29:54388 - 32986 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000249833s
[INFO] 10.244.0.29:54388 - 43998 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.00031875s
[INFO] 10.244.0.29:39001 - 55307 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000243292s
[INFO] 10.244.0.29:39001 - 28424 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000311583s
[INFO] 10.244.0.29:60584 - 33587 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00009575s
[INFO] 10.244.0.29:60584 - 29492 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000126209s
[INFO] 10.244.0.29:35137 - 16237 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00025275s
[INFO] 10.244.0.29:35137 - 54121 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000337875s
[INFO] 10.244.0.29:33213 - 34383 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000286666s
[INFO] 10.244.0.29:33213 - 21425 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000373917s
[INFO] 10.244.0.29:59764 - 25722 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000244375s
[INFO] 10.244.0.29:59764 - 42110 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000323959s
[INFO] 10.244.0.29:33747 - 2814 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000260333s
[INFO] 10.244.0.29:33747 - 52977 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000344083s
[INFO] 10.244.0.29:60693 - 54648 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.0002515s
[INFO] 10.244.0.29:60693 - 27524 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000314083s
[INFO] 10.244.0.29:42636 - 12789 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00023575s
[INFO] 10.244.0.29:42636 - 9457 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000315125s
[INFO] 10.244.0.29:57525 - 34597 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000213458s
[INFO] 10.244.0.29:57525 - 40232 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000264125s
[INFO] 10.244.0.29:49773 - 27019 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.00018975s
[INFO] 10.244.0.29:49773 - 50096 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000255875s
[INFO] 10.244.0.29:38332 - 62540 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.0002795s
[INFO] 10.244.0.29:38332 - 28227 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000379625s
[INFO] 10.244.0.29:58894 - 59636 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000230292s
[INFO] 10.244.0.29:58894 - 34032 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000308708s
[INFO] 10.244.0.29:39368 - 32376 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000244958s
[INFO] 10.244.0.29:39368 - 22909 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000319542s
[INFO] 10.244.0.29:51736 - 60305 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000173625s
[INFO] 10.244.0.29:51736 - 662 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000220042s
[INFO] 10.244.0.29:57703 - 13092 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000154542s
[INFO] 10.244.0.29:57703 - 63778 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000194917s
[INFO] 10.244.0.29:46512 - 31465 "A IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 108 0.000270875s
[INFO] 10.244.0.29:46512 - 11989 "AAAA IN awx-postgres-15.awx.svc.cluster.local. udp 55 false 512" NOERROR qr,aa,rd 148 0.000368375s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=arm64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=arm64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_08_19T16_24_58_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 19 Aug 2025 10:54:55 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Sat, 23 Aug 2025 18:47:45 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Sat, 23 Aug 2025 18:46:05 +0000   Fri, 22 Aug 2025 08:40:32 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Sat, 23 Aug 2025 18:46:05 +0000   Fri, 22 Aug 2025 08:40:32 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Sat, 23 Aug 2025 18:46:05 +0000   Fri, 22 Aug 2025 08:40:32 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Sat, 23 Aug 2025 18:46:05 +0000   Fri, 22 Aug 2025 09:07:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                10
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025424Ki
  pods:               110
Allocatable:
  cpu:                10
  ephemeral-storage:  1055761844Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  hugepages-32Mi:     0
  hugepages-64Ki:     0
  memory:             8025424Ki
  pods:               110
System Info:
  Machine ID:                 3bd49b9aab1b4cfd88a7cabd21fb3411
  System UUID:                3bd49b9aab1b4cfd88a7cabd21fb3411
  Boot ID:                    6ee01402-5e3e-4815-a1da-5da158a0c105
  Kernel Version:             6.10.14-linuxkit
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               arm64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (18 in total)
  Namespace                   Name                                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                                ------------  ----------  ---------------  -------------  ---
  awx                         awx-operator-controller-manager-799967c78c-b7frh    55m (0%)      2 (20%)     96Mi (1%)        1088Mi (13%)   35h
  awx                         awx-postgres-15-0                                   10m (0%)      0 (0%)      64Mi (0%)        0 (0%)         35h
  awx                         awx-task-67fb8f55c5-ljf84                           350m (3%)     0 (0%)      384Mi (4%)       0 (0%)         35h
  awx                         awx-web-5dd647cf67-nhwrb                            250m (2%)     0 (0%)      320Mi (4%)       0 (0%)         35h
  default                     mongo-deployment-5fbf68db69-2f8m8                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  default                     mongo-deployment-5fbf68db69-2rd6s                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  default                     mongo-deployment-5fbf68db69-lmbp2                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  default                     myapp-76578df95d-dpw4j                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         30m
  default                     webapp-deployment-5d89486896-5r8jr                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  default                     webapp-deployment-5d89486896-xh874                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  default                     webapp-deployment-5d89486896-zdpqc                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d6h
  kube-system                 coredns-674b8bbfcf-jb55t                            100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     4d7h
  kube-system                 etcd-minikube                                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         4d7h
  kube-system                 kube-apiserver-minikube                             250m (2%)     0 (0%)      0 (0%)           0 (0%)         4d7h
  kube-system                 kube-controller-manager-minikube                    200m (2%)     0 (0%)      0 (0%)           0 (0%)         4d7h
  kube-system                 kube-proxy-p5z4x                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d7h
  kube-system                 kube-scheduler-minikube                             100m (1%)     0 (0%)      0 (0%)           0 (0%)         4d7h
  kube-system                 storage-provisioner                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         4d7h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests      Limits
  --------           --------      ------
  cpu                1415m (14%)   2 (20%)
  memory             1034Mi (13%)  1258Mi (16%)
  ephemeral-storage  0 (0%)        0 (0%)
  hugepages-1Gi      0 (0%)        0 (0%)
  hugepages-2Mi      0 (0%)        0 (0%)
  hugepages-32Mi     0 (0%)        0 (0%)
  hugepages-64Ki     0 (0%)        0 (0%)
Events:              <none>


==> dmesg <==
[Aug23 16:31] netlink: 'init': attribute type 4 has an invalid length.
[  +0.074462] fakeowner: loading out-of-tree module taints kernel.
[  +1.849190] ICMPv6: NA: 4a:74:c5:57:b4:37 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +1.000921] ICMPv6: NA: 4a:74:c5:57:b4:37 advertised our address fc00:f853:ccd:e793::2 on eth0!
[  +1.001785] ICMPv6: NA: 4a:74:c5:57:b4:37 advertised our address fc00:f853:ccd:e793::2 on eth0!


==> etcd [2e8be834ce8b] <==
{"level":"info","ts":"2025-08-23T17:13:53.804519Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24087}
{"level":"info","ts":"2025-08-23T17:13:53.807709Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":24087,"took":"2.764ms","hash":111654618,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2174976,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:13:53.807758Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":111654618,"revision":24087,"compact-revision":23697}
{"level":"info","ts":"2025-08-23T17:18:53.822097Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24476}
{"level":"info","ts":"2025-08-23T17:18:53.831992Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":24476,"took":"9.061917ms","hash":188634782,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2170880,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:18:53.832045Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":188634782,"revision":24476,"compact-revision":24087}
{"level":"info","ts":"2025-08-23T17:23:53.840062Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":24865}
{"level":"info","ts":"2025-08-23T17:23:53.846270Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":24865,"took":"5.662542ms","hash":1423602549,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2158592,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:23:53.846318Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1423602549,"revision":24865,"compact-revision":24476}
{"level":"info","ts":"2025-08-23T17:27:24.103083Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":30003,"local-member-snapshot-index":20002,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-08-23T17:27:24.116421Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":30003}
{"level":"info","ts":"2025-08-23T17:27:24.116522Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":25003}
{"level":"info","ts":"2025-08-23T17:28:53.846839Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25255}
{"level":"info","ts":"2025-08-23T17:28:53.874996Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":25255,"took":"27.863584ms","hash":1863275223,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2166784,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:28:53.875043Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1863275223,"revision":25255,"compact-revision":24865}
{"level":"info","ts":"2025-08-23T17:33:53.856384Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":25644}
{"level":"info","ts":"2025-08-23T17:33:53.876021Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":25644,"took":"19.011958ms","hash":3752502491,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2158592,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:33:53.876068Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3752502491,"revision":25644,"compact-revision":25255}
{"level":"info","ts":"2025-08-23T17:38:53.868790Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26034}
{"level":"info","ts":"2025-08-23T17:38:53.875744Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":26034,"took":"6.734208ms","hash":1121709510,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2191360,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:38:53.875784Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1121709510,"revision":26034,"compact-revision":25644}
{"level":"info","ts":"2025-08-23T17:43:53.877893Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26423}
{"level":"info","ts":"2025-08-23T17:43:53.889634Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":26423,"took":"11.295417ms","hash":4113812874,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2220032,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:43:53.889667Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4113812874,"revision":26423,"compact-revision":26034}
{"level":"info","ts":"2025-08-23T17:48:53.889547Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":26812}
{"level":"info","ts":"2025-08-23T17:48:53.894651Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":26812,"took":"4.509625ms","hash":393933481,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2179072,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T17:48:53.894704Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":393933481,"revision":26812,"compact-revision":26423}
{"level":"info","ts":"2025-08-23T17:53:53.900759Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27202}
{"level":"info","ts":"2025-08-23T17:53:53.905450Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":27202,"took":"4.015291ms","hash":833145804,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2138112,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T17:53:53.905499Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":833145804,"revision":27202,"compact-revision":26812}
{"level":"info","ts":"2025-08-23T17:58:53.910188Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27590}
{"level":"info","ts":"2025-08-23T17:58:53.925203Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":27590,"took":"14.489959ms","hash":628437841,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2109440,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T17:58:53.925248Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":628437841,"revision":27590,"compact-revision":27202}
{"level":"info","ts":"2025-08-23T18:03:53.918617Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":27983}
{"level":"info","ts":"2025-08-23T18:03:53.928253Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":27983,"took":"8.425375ms","hash":1062369890,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":2273280,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-08-23T18:03:53.928312Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1062369890,"revision":27983,"compact-revision":27590}
{"level":"info","ts":"2025-08-23T18:08:53.928785Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28372}
{"level":"info","ts":"2025-08-23T18:08:53.934850Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":28372,"took":"5.597ms","hash":1068213580,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1978368,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-23T18:08:53.934895Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1068213580,"revision":28372,"compact-revision":27983}
{"level":"info","ts":"2025-08-23T18:13:53.937474Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":28760}
{"level":"info","ts":"2025-08-23T18:13:53.942230Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":28760,"took":"4.259875ms","hash":2641898844,"current-db-size-bytes":3756032,"current-db-size":"3.8 MB","current-db-size-in-use-bytes":1990656,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-23T18:13:53.942277Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2641898844,"revision":28760,"compact-revision":28372}
{"level":"info","ts":"2025-08-23T18:18:53.944047Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29149}
{"level":"info","ts":"2025-08-23T18:18:53.971373Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":29149,"took":"26.7975ms","hash":3076147650,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2154496,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T18:18:53.971418Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3076147650,"revision":29149,"compact-revision":28760}
{"level":"info","ts":"2025-08-23T18:23:53.953087Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29569}
{"level":"info","ts":"2025-08-23T18:23:53.966397Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":29569,"took":"12.789208ms","hash":3592528551,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2215936,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-23T18:23:53.966441Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3592528551,"revision":29569,"compact-revision":29149}
{"level":"info","ts":"2025-08-23T18:28:53.962978Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":29957}
{"level":"info","ts":"2025-08-23T18:28:53.975179Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":29957,"took":"11.705583ms","hash":1952555312,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2088960,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T18:28:53.975234Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1952555312,"revision":29957,"compact-revision":29569}
{"level":"info","ts":"2025-08-23T18:33:53.975013Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30349}
{"level":"info","ts":"2025-08-23T18:33:53.988645Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":30349,"took":"13.142125ms","hash":1513106739,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2093056,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T18:33:53.988693Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1513106739,"revision":30349,"compact-revision":29957}
{"level":"info","ts":"2025-08-23T18:38:53.978455Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":30739}
{"level":"info","ts":"2025-08-23T18:38:53.995548Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":30739,"took":"16.595542ms","hash":3275451225,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2084864,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T18:38:53.995592Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3275451225,"revision":30739,"compact-revision":30349}
{"level":"info","ts":"2025-08-23T18:43:53.987194Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":31135}
{"level":"info","ts":"2025-08-23T18:43:53.993061Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":31135,"took":"5.368375ms","hash":392507905,"current-db-size-bytes":3854336,"current-db-size":"3.9 MB","current-db-size-in-use-bytes":2056192,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-23T18:43:53.993108Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":392507905,"revision":31135,"compact-revision":30739}


==> etcd [468b6866ce6e] <==
{"level":"info","ts":"2025-08-22T09:40:25.068775Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15141}
{"level":"info","ts":"2025-08-22T09:40:25.086590Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":15141,"took":"15.725833ms","hash":338214912,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2240512,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-22T09:40:25.086657Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":338214912,"revision":15141,"compact-revision":14749}
{"level":"info","ts":"2025-08-22T09:45:25.078033Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15531}
{"level":"info","ts":"2025-08-22T09:45:25.123193Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":15531,"took":"33.377958ms","hash":556197287,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2207744,"current-db-size-in-use":"2.2 MB"}
{"level":"info","ts":"2025-08-22T09:45:25.123223Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":556197287,"revision":15531,"compact-revision":15141}
{"level":"info","ts":"2025-08-22T09:50:25.092048Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":15920}
{"level":"info","ts":"2025-08-22T09:50:25.128864Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":15920,"took":"29.685792ms","hash":2766057862,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2347008,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-08-22T09:50:25.128925Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2766057862,"revision":15920,"compact-revision":15531}
{"level":"info","ts":"2025-08-22T09:55:25.102675Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16309}
{"level":"info","ts":"2025-08-22T09:55:25.133054Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16309,"took":"24.80825ms","hash":1599676381,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2080768,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-22T09:55:25.133103Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1599676381,"revision":16309,"compact-revision":15920}
{"level":"info","ts":"2025-08-22T09:57:46.910371Z","caller":"etcdserver/server.go:1476","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000}
{"level":"info","ts":"2025-08-22T09:57:46.922949Z","caller":"etcdserver/server.go:2539","msg":"saved snapshot","snapshot-index":20002}
{"level":"info","ts":"2025-08-22T09:57:46.923074Z","caller":"etcdserver/server.go:2569","msg":"compacted Raft logs","compact-index":15002}
{"level":"info","ts":"2025-08-22T10:00:25.112752Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":16698}
{"level":"info","ts":"2025-08-22T10:00:25.131772Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":16698,"took":"14.927583ms","hash":1719029304,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2052096,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2025-08-22T10:00:25.131889Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1719029304,"revision":16698,"compact-revision":16309}
{"level":"info","ts":"2025-08-22T10:05:25.123139Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17083}
{"level":"info","ts":"2025-08-22T10:05:25.161861Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17083,"took":"32.557875ms","hash":3394978174,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1957888,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:05:25.161899Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3394978174,"revision":17083,"compact-revision":16698}
{"level":"info","ts":"2025-08-22T10:10:25.130252Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17471}
{"level":"info","ts":"2025-08-22T10:10:25.157794Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17471,"took":"23.628792ms","hash":3355702801,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1994752,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:10:25.157984Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3355702801,"revision":17471,"compact-revision":17083}
{"level":"info","ts":"2025-08-22T10:15:25.141163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":17860}
{"level":"info","ts":"2025-08-22T10:15:25.162657Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":17860,"took":"17.504959ms","hash":2180408048,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2007040,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:15:25.162742Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2180408048,"revision":17860,"compact-revision":17471}
{"level":"info","ts":"2025-08-22T10:20:25.155607Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18247}
{"level":"info","ts":"2025-08-22T10:20:25.189910Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":18247,"took":"28.46225ms","hash":593856300,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":2043904,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:20:25.189954Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":593856300,"revision":18247,"compact-revision":17860}
{"level":"info","ts":"2025-08-22T10:25:25.171042Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":18631}
{"level":"info","ts":"2025-08-22T10:25:25.194191Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":18631,"took":"20.586792ms","hash":4152406797,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1953792,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:25:25.194238Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4152406797,"revision":18631,"compact-revision":18247}
{"level":"info","ts":"2025-08-22T10:30:25.185592Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19014}
{"level":"info","ts":"2025-08-22T10:30:25.225561Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":19014,"took":"36.394125ms","hash":275731512,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1982464,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:30:25.225776Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":275731512,"revision":19014,"compact-revision":18631}
{"level":"info","ts":"2025-08-22T10:35:25.204634Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19395}
{"level":"info","ts":"2025-08-22T10:35:25.246198Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":19395,"took":"38.574375ms","hash":521150666,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1986560,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:35:25.246331Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":521150666,"revision":19395,"compact-revision":19014}
{"level":"info","ts":"2025-08-22T10:40:25.219163Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":19777}
{"level":"info","ts":"2025-08-22T10:40:25.246332Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":19777,"took":"24.252083ms","hash":4280005939,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-08-22T10:40:25.246609Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":4280005939,"revision":19777,"compact-revision":19395}
{"level":"info","ts":"2025-08-22T10:45:25.232064Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20157}
{"level":"info","ts":"2025-08-22T10:45:25.263988Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":20157,"took":"29.047167ms","hash":2610332506,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1957888,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T10:45:25.264029Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2610332506,"revision":20157,"compact-revision":19777}
{"level":"info","ts":"2025-08-22T10:50:25.249617Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20538}
{"level":"info","ts":"2025-08-22T10:50:25.279454Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":20538,"took":"27.365125ms","hash":82096439,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1912832,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-08-22T10:50:25.279495Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":82096439,"revision":20538,"compact-revision":20157}
{"level":"info","ts":"2025-08-22T10:55:25.259511Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":20919}
{"level":"info","ts":"2025-08-22T10:55:25.280192Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":20919,"took":"18.368459ms","hash":688775107,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1896448,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-08-22T10:55:25.280292Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":688775107,"revision":20919,"compact-revision":20538}
{"level":"info","ts":"2025-08-22T11:00:25.269742Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21299}
{"level":"info","ts":"2025-08-22T11:00:25.288720Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":21299,"took":"17.33075ms","hash":3107927380,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1953792,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T11:00:25.288740Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3107927380,"revision":21299,"compact-revision":20919}
{"level":"info","ts":"2025-08-22T11:05:25.281959Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":21681}
{"level":"info","ts":"2025-08-22T11:05:25.303077Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":21681,"took":"19.219333ms","hash":2603841942,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1986560,"current-db-size-in-use":"2.0 MB"}
{"level":"info","ts":"2025-08-22T11:05:25.303117Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2603841942,"revision":21681,"compact-revision":21299}
{"level":"info","ts":"2025-08-22T11:10:25.292615Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":22061}
{"level":"info","ts":"2025-08-22T11:10:25.311565Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":22061,"took":"17.19775ms","hash":1017225103,"current-db-size-bytes":3653632,"current-db-size":"3.7 MB","current-db-size-in-use-bytes":1933312,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2025-08-22T11:10:25.311608Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1017225103,"revision":22061,"compact-revision":21681}


==> kernel <==
 18:47:50 up  2:16,  0 users,  load average: 4.81, 4.65, 4.57
Linux minikube 6.10.14-linuxkit #1 SMP Sat May 17 08:28:57 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [cd8c201fc610] <==
E0822 08:48:51.611942       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="24.286125ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
I0822 08:50:26.229870       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0822 08:54:52.485018       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 08:54:52.486799       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 08:54:52.598577       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.598566       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.605951       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.606052       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.693332       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.700020       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="281.555375ms" method="GET" path="/apis/coordination.k8s.io/v1/namespaces/awx/leases/awx-operator" result=null
E0822 08:54:52.702327       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 08:54:52.711416       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="300.670625ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0822 08:54:52.899177       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="184.225917ms" method="GET" path="/readyz" result=null
I0822 09:00:26.231371       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
E0822 09:00:53.217088       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:00:53.216967       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:00:53.497361       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.516560       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.518339       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.520842       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.592088       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.594594       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:00:53.599407       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="395.743542ms" method="PUT" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0822 09:00:53.599440       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="395.76875ms" method="GET" path="/apis/coordination.k8s.io/v1/namespaces/awx/leases/awx-operator" result=null
E0822 09:07:32.299963       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:07:32.306240       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:07:32.306384       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.308405       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.310968       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
W0822 09:07:32.311579       1 logging.go:55] [core] [Channel #319 SubChannel #320]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: operation was canceled"
E0822 09:07:32.311603       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.315398       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="92.196708ms" method="GET" path="/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath" result=null
E0822 09:07:32.388566       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.394476       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.397356       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:07:32.397519       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.398347       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="175.965ms" method="GET" path="/api" result=null
E0822 09:07:32.404567       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.404642       1 authentication.go:75] "Unable to authenticate the request" err="[invalid bearer token, context canceled]"
E0822 09:07:32.404700       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.410089       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.488963       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.488983       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E0822 09:07:32.492523       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="102.951667ms" method="GET" path="/api" result=null
E0822 09:07:32.495742       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="181.356042ms" method="GET" path="/apis/coordination.k8s.io/v1/namespaces/awx/leases/awx-operator" result=null
I0822 09:07:33.118730       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 09:10:26.241495       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 09:13:12.411428       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0822 09:20:26.239456       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 09:30:26.243179       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 09:40:26.246768       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 09:50:26.247434       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:00:26.251159       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:10:26.252643       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:20:26.256684       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:30:26.263068       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:40:26.265218       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 10:50:26.267861       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 11:00:26.271754       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0822 11:10:26.276073       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [e07a5107179f] <==
I0823 16:53:54.141754       1 establishing_controller.go:81] Starting EstablishingController
I0823 16:53:54.142467       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0823 16:53:54.142493       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0823 16:53:54.142536       1 crd_finalizer.go:269] Starting CRDFinalizer
I0823 16:53:54.142615       1 controller.go:142] Starting OpenAPI controller
I0823 16:53:54.142627       1 controller.go:90] Starting OpenAPI V3 controller
I0823 16:53:54.141509       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0823 16:53:54.141754       1 naming_controller.go:299] Starting NamingConditionController
I0823 16:53:54.142251       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0823 16:53:54.156145       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 16:53:54.227289       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0823 16:53:54.234455       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0823 16:53:54.234482       1 policy_source.go:240] refreshing policies
I0823 16:53:54.240926       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0823 16:53:54.240959       1 cache.go:39] Caches are synced for LocalAvailability controller
I0823 16:53:54.241035       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0823 16:53:54.241198       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0823 16:53:54.241267       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0823 16:53:54.241284       1 aggregator.go:171] initial CRD sync complete...
I0823 16:53:54.241301       1 autoregister_controller.go:144] Starting autoregister controller
I0823 16:53:54.241307       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0823 16:53:54.241311       1 cache.go:39] Caches are synced for autoregister controller
I0823 16:53:54.241442       1 handler.go:288] Adding GroupVersion awx.ansible.com v1beta1 to ResourceManager
I0823 16:53:54.241453       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0823 16:53:54.241459       1 handler.go:288] Adding GroupVersion awx.ansible.com v1alpha1 to ResourceManager
I0823 16:53:54.241476       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0823 16:53:54.241534       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0823 16:53:54.241535       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0823 16:53:54.241619       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0823 16:53:54.241628       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0823 16:53:54.243556       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0823 16:53:54.247380       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0823 16:53:54.252697       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
E0823 16:53:54.325628       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: ab19487c-fc1e-4efa-89af-48d0a43a8441, UID in object meta: "
I0823 16:53:55.146643       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0823 16:53:56.195534       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0823 16:53:57.629464       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0823 16:53:57.743665       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 16:53:57.926122       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0823 16:53:58.035780       1 controller.go:667] quota admission added evaluator for: statefulsets.apps
I0823 16:53:58.131525       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0823 16:53:58.240473       1 controller.go:667] quota admission added evaluator for: endpoints
I0823 16:53:58.337871       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0823 16:54:48.266617       1 controller.go:667] quota admission added evaluator for: awxs.awx.ansible.com
I0823 16:54:59.355045       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0823 16:54:59.463732       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0823 16:55:04.073968       1 controller.go:667] quota admission added evaluator for: jobs.batch
I0823 17:03:54.179868       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 17:13:54.182689       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 17:23:54.198027       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 17:33:54.206491       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 17:43:54.217735       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 17:53:54.226807       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:03:54.236565       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:13:54.243801       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:17:03.453692       1 alloc.go:328] "allocated clusterIPs" service="default/myapp" clusterIPs={"IPv4":"10.104.181.145"}
I0823 18:17:03.478155       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:23:54.251370       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:33:54.258274       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0823 18:43:54.259912       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [3b86c986d23b] <==
I0822 08:40:44.185552       1 controller_utils.go:151] "Failed to update status for pod" logger="node-lifecycle-controller" pod="kube-system/kube-apiserver-minikube" err="Operation cannot be fulfilled on pods \"kube-apiserver-minikube\": the object has been modified; please apply your changes to the latest version and try again"
E0822 08:40:44.196676       1 node_lifecycle_controller.go:747] "Unhandled Error" err="unable to mark all pods NotReady on node minikube: [Operation cannot be fulfilled on pods \"kube-scheduler-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"coredns-674b8bbfcf-jb55t\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"etcd-minikube\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on pods \"kube-apiserver-minikube\": the object has been modified; please apply your changes to the latest version and try again]; queuing for retry" logger="UnhandledError"
I0822 08:40:49.203887       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"
I0822 09:07:28.300522       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.300586       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.300555       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.300165       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.FlowSchema" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.307963       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.300675       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.309734       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Secret" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.310439       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceQuota" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.302981       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CertificateSigningRequest" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.300431       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390085       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Lease" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390202       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390271       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RoleBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390318       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IPAddress" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390572       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390620       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390678       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceAccount" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390731       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390812       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Ingress" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390955       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391052       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicy" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391113       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.IngressClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391190       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodTemplate" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391297       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DaemonSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.390476       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingAdmissionPolicyBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391636       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Deployment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391754       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityLevelConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391867       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.391934       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Job" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.392019       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ValidatingWebhookConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.392950       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.393158       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.393852       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.394120       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.394712       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v2.HorizontalPodAutoscaler" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.395121       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.LimitRange" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.395292       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Role" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.395355       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRole" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396303       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.NetworkPolicy" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396368       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396490       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.RuntimeClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396571       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396623       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396692       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396744       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396842       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Endpoints" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396890       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.396942       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.MutatingWebhookConfiguration" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.395441       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/metadata/metadatainformer/informer.go:138" type="*v1.PartialObjectMetadata" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.403598       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ClusterRoleBinding" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.403681       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PriorityClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.403732       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.403782       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CronJob" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.403811       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ControllerRevision" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E0822 09:07:33.114729       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I0822 09:07:33.137745       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I0822 09:07:43.143545       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"


==> kube-controller-manager [5159343e96f0] <==
I0823 16:53:57.337140       1 controllermanager.go:778] "Started controller" controller="legacy-serviceaccount-token-cleaner-controller"
I0823 16:53:57.337151       1 controllermanager.go:741] "Warning: controller is disabled" controller="selinux-warning-controller"
I0823 16:53:57.337186       1 legacy_serviceaccount_token_cleaner.go:103] "Starting legacy service account token cleaner controller" logger="legacy-serviceaccount-token-cleaner-controller"
I0823 16:53:57.337195       1 shared_informer.go:350] "Waiting for caches to sync" controller="legacy-service-account-token-cleaner"
I0823 16:53:57.424784       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0823 16:53:57.426333       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0823 16:53:57.428965       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0823 16:53:57.435516       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0823 16:53:57.438283       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0823 16:53:57.450776       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0823 16:53:57.457433       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0823 16:53:57.457618       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0823 16:53:57.459559       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0823 16:53:57.465620       1 shared_informer.go:357] "Caches are synced" controller="node"
I0823 16:53:57.465654       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0823 16:53:57.465668       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0823 16:53:57.465673       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0823 16:53:57.465675       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0823 16:53:57.467597       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0823 16:53:57.467763       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0823 16:53:57.472568       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0823 16:53:57.523062       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0823 16:53:57.523064       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0823 16:53:57.523229       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0823 16:53:57.523372       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0823 16:53:57.523613       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0823 16:53:57.523624       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0823 16:53:57.523754       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0823 16:53:57.525186       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0823 16:53:57.526596       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0823 16:53:57.529127       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0823 16:53:57.529263       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0823 16:53:57.529561       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0823 16:53:57.532405       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0823 16:53:57.532448       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0823 16:53:57.537241       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0823 16:53:57.624064       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0823 16:53:57.624105       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0823 16:53:57.624180       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0823 16:53:57.624414       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0823 16:53:57.724564       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0823 16:53:57.724635       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0823 16:53:57.724643       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0823 16:53:57.724646       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0823 16:53:57.724679       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0823 16:53:57.827467       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0823 16:53:57.827887       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0823 16:53:57.827994       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0823 16:53:57.828054       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0823 16:53:57.828081       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0823 16:53:57.829516       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0823 16:53:57.923594       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0823 16:53:57.923743       1 shared_informer.go:357] "Caches are synced" controller="job"
I0823 16:53:57.923744       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0823 16:53:57.923895       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0823 16:53:58.251226       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0823 16:53:58.323817       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0823 16:53:58.323869       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0823 16:53:58.323886       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0823 16:53:58.439855       1 endpointslice_controller.go:344] "Error syncing endpoint slices for service, retrying" logger="endpointslice-controller" key="default/mongo-service" err="EndpointSlice informer cache is out of date"


==> kube-proxy [1bd5384b3d39] <==
I0822 06:40:07.299605       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0822 06:40:07.300295       1 config.go:440] "Starting serviceCIDR config controller"
I0822 06:40:07.300307       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0822 06:40:07.300329       1 config.go:329] "Starting node config controller"
I0822 06:40:07.300332       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0822 06:40:07.399634       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0822 06:40:07.401554       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0822 06:40:07.401565       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0822 06:40:07.401571       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0822 07:48:57.107264       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 07:48:59.508308       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 07:49:00.889294       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 07:49:01.705634       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 07:49:43.502565       1 request.go:752] "Waited before sending request" delay="3.205006584s" reason="client-side throttling, not priority and fairness" verb="GET" URL="https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=8973"
E0822 07:50:15.883114       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8984\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 07:50:30.700343       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8980\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 07:50:33.405079       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8993\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 07:51:59.508912       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=8973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 07:52:31.508091       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8984\": dial tcp: lookup control-plane.minikube.internal: i/o timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 07:52:42.494029       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8980\": dial tcp: lookup control-plane.minikube.internal: i/o timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 07:53:02.395225       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8993\": dial tcp: lookup control-plane.minikube.internal: i/o timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 07:53:40.509751       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8984\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 07:53:48.779145       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=8973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 07:53:55.004801       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8993\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 07:53:59.986057       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8980\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 07:54:56.508050       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8984\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 07:55:20.404094       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=8973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 07:55:28.781028       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8980\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 07:55:31.686635       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8993\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 07:55:43.980649       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=8984\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 07:55:43.982777       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=8973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0822 08:34:44.721986       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 08:34:47.915826       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 08:34:46.905141       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
E0822 08:34:50.216455       1 reflector.go:200] "Failed to watch" err="Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?allowWatchBookmarks=true&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974&timeout=9m21s&timeoutSeconds=561&watch=true\": http2: client connection lost" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:35:20.220650       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:35:20.618120       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:35:26.019929       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:35:30.317989       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 08:36:05.389922       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:36:12.524865       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:36:13.214316       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:36:26.318262       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
I0822 08:36:32.513953       1 request.go:752] "Waited before sending request" delay="1.392109084s" reason="client-side throttling, not priority and fairness" verb="GET" URL="https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974"
E0822 08:37:07.986175       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:37:12.705580       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:37:29.213195       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:37:43.404526       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 08:38:29.400636       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:38:44.611350       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:38:52.221221       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:39:12.606677       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ServiceCIDR: Get \"https://control-plane.minikube.internal:8443/apis/networking.k8s.io/v1/servicecidrs?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR"
E0822 08:39:28.013851       1 reflector.go:200] "Failed to watch" err="failed to list *v1.EndpointSlice: Get \"https://control-plane.minikube.internal:8443/apis/discovery.k8s.io/v1/endpointslices?labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice"
E0822 08:39:42.713677       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:39:45.184408       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://control-plane.minikube.internal:8443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&labelSelector=%21service.kubernetes.io%2Fheadless%2C%21service.kubernetes.io%2Fservice-proxy-name&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:40:17.908542       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&resourceVersion=11974\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
I0822 09:07:26.212496       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.EndpointSlice" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:26.099750       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:26.099912       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:26.393289       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ServiceCIDR" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"


==> kube-proxy [90c7305a485c] <==
I0823 16:53:58.725711       1 server_linux.go:63] "Using iptables proxy"
I0823 16:53:59.140059       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0823 16:53:59.140189       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0823 16:53:59.335803       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0823 16:53:59.335855       1 server_linux.go:145] "Using iptables Proxier"
I0823 16:53:59.341644       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I0823 16:53:59.344380       1 server.go:516] "Version info" version="v1.33.1"
I0823 16:53:59.344402       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0823 16:53:59.347935       1 config.go:199] "Starting service config controller"
I0823 16:53:59.422461       1 config.go:329] "Starting node config controller"
I0823 16:53:59.425408       1 config.go:105] "Starting endpoint slice config controller"
I0823 16:53:59.425659       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0823 16:53:59.426809       1 config.go:440] "Starting serviceCIDR config controller"
I0823 16:53:59.427240       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0823 16:53:59.427732       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0823 16:53:59.427755       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0823 16:53:59.528236       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0823 16:53:59.528269       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0823 16:53:59.531247       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0823 16:53:59.531690       1 shared_informer.go:357] "Caches are synced" controller="node config"


==> kube-scheduler [13c4d9508b27] <==
E0822 08:37:51.916789       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0822 08:37:53.814247       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0822 08:37:56.509848       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0822 08:37:55.400411       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0822 08:37:56.786964       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0822 08:38:01.219247       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0822 08:38:01.915379       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
I0822 08:38:28.002508       1 request.go:752] "Waited before sending request" delay="2.097237042s" reason="client-side throttling, not priority and fairness" verb="GET" URL="https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=11974"
I0822 08:38:54.505803       1 request.go:752] "Waited before sending request" delay="1.277898375s" reason="client-side throttling, not priority and fairness" verb="GET" URL="https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=11974"
E0822 08:39:10.818394       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:39:27.291596       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0822 08:39:27.389205       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0822 08:39:27.414773       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0822 08:39:27.292256       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0822 08:39:27.411274       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:27.485528       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:27.411550       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0822 08:39:27.411310       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=11973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0822 08:39:27.487327       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0822 08:39:27.487826       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:39:27.487848       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0822 08:39:27.415700       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:27.615646       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0822 08:39:27.683216       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0822 08:39:41.991991       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: Get \"https://192.168.49.2:8443/api/v1/persistentvolumeclaims?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0822 08:39:42.720817       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: Get \"https://192.168.49.2:8443/api/v1/namespaces?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0822 08:39:45.206137       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:45.194056       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: Get \"https://192.168.49.2:8443/apis/policy/v1/poddisruptionbudgets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0822 08:39:45.206087       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: Get \"https://192.168.49.2:8443/apis/apps/v1/statefulsets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0822 08:39:45.194183       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: Get \"https://192.168.49.2:8443/api/v1/services?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0822 08:39:45.205409       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0822 08:39:45.206217       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:45.206119       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/storageclasses?resourceVersion=11973\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0822 08:39:45.206165       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: Get \"https://192.168.49.2:8443/api/v1/pods?fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0822 08:39:45.206193       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0822 08:39:45.206175       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: Get \"https://192.168.49.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%3Dextension-apiserver-authentication&resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap"
E0822 08:39:45.206201       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/volumeattachments?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0822 08:39:45.894645       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csinodes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0822 08:39:45.894627       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0822 08:39:45.894608       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: Get \"https://192.168.49.2:8443/api/v1/persistentvolumes?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0822 08:39:45.894640       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: Get \"https://192.168.49.2:8443/apis/apps/v1/replicasets?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0822 08:39:46.509646       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: Get \"https://192.168.49.2:8443/api/v1/replicationcontrollers?resourceVersion=11974\": net/http: TLS handshake timeout" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
I0822 09:07:14.588897       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:14.525009       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:14.726826       1 reflector.go:556] "Warning: watch ended with error" reflector="runtime/asm_arm64.s:1223" type="*v1.ConfigMap" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:27.814146       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.211379       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.213200       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.215389       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.217163       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.220141       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.291242       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.219930       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.112782       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.211379       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.211182       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.213315       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.212874       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.296789       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"
I0822 09:07:28.296929       1 reflector.go:556] "Warning: watch ended with error" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment" err="an error on the server (\"unable to decode an event from the watch stream: http2: client connection lost\") has prevented the request from succeeding"


==> kube-scheduler [453cf426b4e1] <==
I0823 16:53:53.251482       1 serving.go:386] Generated self-signed cert in-memory
W0823 16:53:54.153475       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0823 16:53:54.153524       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0823 16:53:54.153538       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0823 16:53:54.153546       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0823 16:53:54.163141       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0823 16:53:54.163169       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0823 16:53:54.165517       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0823 16:53:54.165959       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0823 16:53:54.166630       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0823 16:53:54.167771       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0823 16:53:54.268474       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.158570    1668 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: Get \"https://192.168.49.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.188564    1668 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: Get \"https://192.168.49.2:8443/api/v1/nodes?fieldSelector=metadata.name%3Dminikube&limit=500&resourceVersion=0\": dial tcp 192.168.49.2:8443: connect: connection refused" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.536884    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.540412    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.544074    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:53 minikube kubelet[1668]: E0823 16:53:53.556759    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:53 minikube kubelet[1668]: I0823 16:53:53.836777    1668 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: I0823 16:53:54.250531    1668 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: I0823 16:53:54.250584    1668 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.250597    1668 kubelet_node_status.go:548] "Error updating node status, will retry" err="error getting node \"minikube\": node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: I0823 16:53:54.251479    1668 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Aug 23 16:53:54 minikube kubelet[1668]: I0823 16:53:54.253243    1668 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.325648    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.426409    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.526831    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.571907    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.572056    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.572182    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.572271    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.627364    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.728480    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.829640    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:54 minikube kubelet[1668]: E0823 16:53:54.930507    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.031446    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.132690    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.233590    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.334917    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.435664    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.536966    1668 kubelet_node_status.go:466] "Error getting the current node from lister" err="node \"minikube\" not found"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.580920    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.581241    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.581375    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.581678    1668 kubelet.go:3305] "No need to create a mirror pod, since failed to get node info from the cluster" err="node \"minikube\" not found" node="minikube"
Aug 23 16:53:55 minikube kubelet[1668]: I0823 16:53:55.595490    1668 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.600358    1668 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: I0823 16:53:55.600374    1668 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.602663    1668 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: I0823 16:53:55.602675    1668 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.606368    1668 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: I0823 16:53:55.606534    1668 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Aug 23 16:53:55 minikube kubelet[1668]: E0823 16:53:55.610121    1668 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.329234    1668 apiserver.go:52] "Watching apiserver"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.399320    1668 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.476822    1668 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/b389e27f-fa60-415b-8d4c-873fdc875823-xtables-lock\") pod \"kube-proxy-p5z4x\" (UID: \"b389e27f-fa60-415b-8d4c-873fdc875823\") " pod="kube-system/kube-proxy-p5z4x"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.476915    1668 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/d4e9f929-1c9c-4f31-9301-27c754106940-tmp\") pod \"storage-provisioner\" (UID: \"d4e9f929-1c9c-4f31-9301-27c754106940\") " pod="kube-system/storage-provisioner"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.476976    1668 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/b389e27f-fa60-415b-8d4c-873fdc875823-lib-modules\") pod \"kube-proxy-p5z4x\" (UID: \"b389e27f-fa60-415b-8d4c-873fdc875823\") " pod="kube-system/kube-proxy-p5z4x"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.477346    1668 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-cb6ddb70-8ce5-4567-a9dd-70aaebbec259\" (UniqueName: \"kubernetes.io/host-path/0b24baba-56a1-42c2-8130-bcbab354347c-pvc-cb6ddb70-8ce5-4567-a9dd-70aaebbec259\") pod \"awx-postgres-15-0\" (UID: \"0b24baba-56a1-42c2-8130-bcbab354347c\") " pod="awx/awx-postgres-15-0"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.727505    1668 scope.go:117] "RemoveContainer" containerID="b7ec7b9a63080ecf5d1e03edcfd468594c03710d477407f7ebdaff701df13e85"
Aug 23 16:53:56 minikube kubelet[1668]: I0823 16:53:56.763948    1668 scope.go:117] "RemoveContainer" containerID="e7ac53ff96fae81ad58b454fef6017b8c90a67bedbe317b57aa05ca3a17a863f"
Aug 23 16:54:01 minikube kubelet[1668]: I0823 16:54:01.926399    1668 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Aug 23 16:54:06 minikube kubelet[1668]: I0823 16:54:06.468604    1668 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Aug 23 16:54:28 minikube kubelet[1668]: I0823 16:54:28.059478    1668 scope.go:117] "RemoveContainer" containerID="99c479a009235145ff5cf0c34be46ca24130405d872b749639e450a5ec6ace9f"
Aug 23 16:54:28 minikube kubelet[1668]: I0823 16:54:28.060080    1668 scope.go:117] "RemoveContainer" containerID="04348fb9f3b4ea38fac98a97ea55fff2b863ef74a0e9c8be6399223ab2b7d05e"
Aug 23 16:54:28 minikube kubelet[1668]: E0823 16:54:28.060364    1668 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(d4e9f929-1c9c-4f31-9301-27c754106940)\"" pod="kube-system/storage-provisioner" podUID="d4e9f929-1c9c-4f31-9301-27c754106940"
Aug 23 16:54:39 minikube kubelet[1668]: I0823 16:54:39.308311    1668 scope.go:117] "RemoveContainer" containerID="04348fb9f3b4ea38fac98a97ea55fff2b863ef74a0e9c8be6399223ab2b7d05e"
Aug 23 16:54:46 minikube kubelet[1668]: I0823 16:54:46.300997    1668 scope.go:117] "RemoveContainer" containerID="c0c42f1672248ae04da1bcf2784d96050c0e5a90b5a513e2c3c45a989994e74c"
Aug 23 16:54:46 minikube kubelet[1668]: I0823 16:54:46.301033    1668 scope.go:117] "RemoveContainer" containerID="a7764c73b9236688b75bcde79007e78349c52dffd45e6511b0ebee3e607e97f3"
Aug 23 16:54:46 minikube kubelet[1668]: I0823 16:54:46.301039    1668 scope.go:117] "RemoveContainer" containerID="5ba00ea25a4a1e872606e5c257229d3094e11f116605a27b5e7ed3424be8e284"
Aug 23 16:54:46 minikube kubelet[1668]: I0823 16:54:46.301043    1668 scope.go:117] "RemoveContainer" containerID="d4b19fb0d1f6e1f5a4d863690b410a5e452f03b87b31c1db4482bd4cd9c488f5"
Aug 23 18:17:03 minikube kubelet[1668]: I0823 18:17:03.564164    1668 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hxcxm\" (UniqueName: \"kubernetes.io/projected/95699794-b173-4904-8c1b-98fb5ef3c34f-kube-api-access-hxcxm\") pod \"myapp-76578df95d-dpw4j\" (UID: \"95699794-b173-4904-8c1b-98fb5ef3c34f\") " pod="default/myapp-76578df95d-dpw4j"


==> storage-provisioner [04348fb9f3b4] <==
I0823 16:53:57.726322       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0823 16:54:27.736158       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout


==> storage-provisioner [ff8b34f7931f] <==
W0823 18:46:51.128447       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:51.133436       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:53.139162       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:53.143465       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:55.147888       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:55.152088       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:57.157985       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:57.163618       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:59.169154       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:46:59.173729       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:01.177691       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:01.183156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:03.187278       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:03.191592       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:05.196726       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:05.200656       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:07.205275       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:07.210866       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:09.214652       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:09.217584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:11.222559       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:11.228763       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:13.233175       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:13.236696       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:15.240880       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:15.245095       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:17.249612       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:17.256875       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:19.258117       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:19.259292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:21.261950       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:21.267520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:23.271319       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:23.274063       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:25.276775       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:25.279967       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:27.287845       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:27.293440       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:29.296628       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:29.298569       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:31.301990       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:31.306745       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:33.310484       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:33.314758       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:35.318612       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:35.321922       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:37.326689       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:37.332156       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:39.334999       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:39.337863       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:41.342912       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:41.348246       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:43.350691       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:43.353069       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:45.356732       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:45.361027       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:47.366215       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:47.373290       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:49.376167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0823 18:47:49.377705       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

